{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "# --------------------------------------------------------\n",
    "# References:\n",
    "# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from json import encoder\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import Block\n",
    "from timm.models.layers import to_2tuple\n",
    "\n",
    "import timm\n",
    "\n",
    "import os, gc, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import librosa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import ASTConfig, ASTFeatureExtractor, ASTModel\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from time import time\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Config and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(log_file='log.txt'):\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    # Logging to file\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    # Logging to console\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def wandb_init(project_name, run_name, config):\n",
    "    config_dict = {\n",
    "        k: v for k, v in config.__dict__.items() if not k.startswith('_') and not callable(v) and k != 'copy'\n",
    "    }\n",
    "    run = wandb.init(project=project_name, name=run_name, config=config_dict)\n",
    "    return run\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.value = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.value = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DRIVE_FOLDER = \".\" #\"/content/drive/MyDrive/Colab Notebooks\"\n",
    "KEEP_COLS = ['category_number', 'common_name', 'audio_length', 'type', 'remarks', 'quality', 'scientific_name', 'mp3_link', 'region']\n",
    "\n",
    "class Config:\n",
    "    # path\n",
    "    dataset_dir = f\"{DRIVE_FOLDER}/Audio_XenoCanto\"\n",
    "    labels_list = f\"{DRIVE_FOLDER}/xeno_labels.csv\"\n",
    "    model_name = \"ast_baseline\"\n",
    "    backbone_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "    # number of classes in the dataset\n",
    "    n_classes = 397 \n",
    "    # Audio parameters\n",
    "    audio_sr = 16000 #Hz\n",
    "    segment_length = 10  #s\n",
    "    fft_window = 0.025 #s\n",
    "    hop_window_length = 0.01 #s\n",
    "    n_mels = 128\n",
    "    low_cut = 1000 #Hz\n",
    "    high_cut = 8000 #Hz\n",
    "    top_db = 100\n",
    "    # Training parameters\n",
    "    batch_size = 4 \n",
    "    num_workers = 0\n",
    "    n_splits = 5\n",
    "    log_dir = f\"{DRIVE_FOLDER}/training_logs\"\n",
    "    max_lr = 1e-5\n",
    "    epochs = 10\n",
    "    weight_decay = 0.01\n",
    "    lr_final_div = 1000\n",
    "    amp = True\n",
    "    grad_accum_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "    print_epoch_freq = 1\n",
    "    print_freq = 200\n",
    "    # model parameters\n",
    "    n_decoder_layers = 6\n",
    "    n_decoder_heads = 6\n",
    "    ff_dim_decoder = 2048\n",
    "    # seed\n",
    "    random_seed = 2046\n",
    "    \n",
    "    @classmethod\n",
    "    def copy(cls):\n",
    "        new_class = type('CustomConfig', (cls,), {k: v for k, v in cls.__dict__.items() if not k.startswith('__') and not callable(v)})\n",
    "        return new_class\n",
    "    \n",
    "config = Config.copy()\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "seed_everything(config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes with less than 2 samples: 72\n",
      "Number of classes in dataset: 728\n",
      "Number of samples: 11171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>category_number</th>\n",
       "      <th>common_name</th>\n",
       "      <th>audio_length</th>\n",
       "      <th>type</th>\n",
       "      <th>remarks</th>\n",
       "      <th>quality</th>\n",
       "      <th>mp3_link</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>region</th>\n",
       "      <th>file_exists</th>\n",
       "      <th>species_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/XC228210-Blue-crowned_Manakin_B_9369_0.wav</td>\n",
       "      <td>XC228210</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:20</td>\n",
       "      <td>call</td>\n",
       "      <td>ID certainty 80%. (Archiv. tape 393 side A tra...</td>\n",
       "      <td>B</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/XC228210-Blue-crowned_Manakin_B_9369_1.wav</td>\n",
       "      <td>XC228210</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:20</td>\n",
       "      <td>call</td>\n",
       "      <td>ID certainty 80%. (Archiv. tape 393 side A tra...</td>\n",
       "      <td>B</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/XC200163-PIPCOR03_0.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/XC200163-PIPCOR03_1.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/XC200163-PIPCOR03_2.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name category_number  \\\n",
       "0  data/XC228210-Blue-crowned_Manakin_B_9369_0.wav        XC228210   \n",
       "1  data/XC228210-Blue-crowned_Manakin_B_9369_1.wav        XC228210   \n",
       "2                     data/XC200163-PIPCOR03_0.wav        XC200163   \n",
       "3                     data/XC200163-PIPCOR03_1.wav        XC200163   \n",
       "4                     data/XC200163-PIPCOR03_2.wav        XC200163   \n",
       "\n",
       "            common_name audio_length        type  \\\n",
       "0  Blue-crowned Manakin         0:20        call   \n",
       "1  Blue-crowned Manakin         0:20        call   \n",
       "2  Blue-crowned Manakin         0:42  call, song   \n",
       "3  Blue-crowned Manakin         0:42  call, song   \n",
       "4  Blue-crowned Manakin         0:42  call, song   \n",
       "\n",
       "                                             remarks quality  \\\n",
       "0  ID certainty 80%. (Archiv. tape 393 side A tra...       B   \n",
       "1  ID certainty 80%. (Archiv. tape 393 side A tra...       B   \n",
       "2  left bank of rio Negro - terra firme forest, w...       C   \n",
       "3  left bank of rio Negro - terra firme forest, w...       C   \n",
       "4  left bank of rio Negro - terra firme forest, w...       C   \n",
       "\n",
       "                                            mp3_link       scientific_name  \\\n",
       "0  //xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...  Lepidothrix_coronata   \n",
       "1  //xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...  Lepidothrix_coronata   \n",
       "2  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "3  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "4  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "\n",
       "     region  file_exists  species_id  \n",
       "0  amazonas         True         329  \n",
       "1  amazonas         True         329  \n",
       "2  amazonas         True         329  \n",
       "3  amazonas         True         329  \n",
       "4  amazonas         True         329  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_audio_meta = pd.read_csv(f\"{config.dataset_dir}/metadata.csv\", nrows=None)\n",
    "df_audio_meta = df_audio_meta.dropna().reset_index(drop=True)\n",
    "\n",
    "# Filter out files that do not exist\n",
    "df_audio_meta['file_exists'] = df_audio_meta['file_name'].apply(lambda x: os.path.exists(f\"{config.dataset_dir}/{x}\"))\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['file_exists']].reset_index(drop=True)\n",
    "\n",
    "# parse scientific names\n",
    "df_audio_meta['scientific_name'] = df_audio_meta['scientific_name'].apply(lambda x: \"_\".join(x.split(\" \")))\n",
    "\n",
    "# drop species with less than 2 samples\n",
    "class_counts = df_audio_meta['scientific_name'].value_counts()\n",
    "print(f\"Number of classes with less than 2 samples: {len(class_counts[class_counts < 2])}\")\n",
    "\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['scientific_name'].isin(class_counts[class_counts > 1].index)].copy().reset_index(drop=True)\n",
    "\n",
    "# encode scientific names to label ids\n",
    "label_ids_list = df_audio_meta['scientific_name'].unique().tolist()\n",
    "label_ids_list.sort()\n",
    "label_to_id = {label: i for i, label in enumerate(label_ids_list)}\n",
    "df_audio_meta['species_id'] = df_audio_meta['scientific_name'].map(label_to_id)\n",
    "\n",
    "# drop samples with no labels\n",
    "df_audio_meta.dropna(subset=['species_id'], inplace=True)\n",
    "df_audio_meta.reset_index(drop=True, inplace=True)\n",
    "df_audio_meta['species_id'] = df_audio_meta['species_id'].astype(int)\n",
    "\n",
    "print(f\"Number of classes in dataset: {df_audio_meta['species_id'].nunique()}\")\n",
    "print(f'Number of samples:', len(df_audio_meta))\n",
    "\n",
    "# save the number of classes in the config\n",
    "config.n_classes = df_audio_meta['species_id'].nunique()\n",
    "\n",
    "df_audio_meta.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils for positional embeddings\n",
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_flexible(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size[0], dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size[1], dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size[0], grid_size[1]])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "class PatchEmbed_org(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.patch_hw = (img_size[1] // patch_size[1], img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        #assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #    f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, backbone_name, contextual_depth=8):\n",
    "        super().__init__()\n",
    "        self.backbone_model = timm.create_model(backbone_name, pretrained=True, num_classes=0)\n",
    "        \n",
    "        # desamble the backbone model\n",
    "        self.patch_embed = getattr(self.backbone_model, 'patch_embed')\n",
    "        self.cls_token = getattr(self.backbone_model, 'cls_token')\n",
    "        self.pos_embed = getattr(self.backbone_model, 'pos_embed')\n",
    "        self.blocks = getattr(self.backbone_model, 'blocks')\n",
    "        self.norm = getattr(self.backbone_model, 'fc_norm')\n",
    "        \n",
    "        self.embed_dim = self.backbone_model.embed_dim\n",
    "        self.num_patches = self.patch_embed.num_patches\n",
    "        self.encoder_depth = len(self.blocks)\n",
    "        self.contextual_depth=contextual_depth\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        # Initialize the weights of each module using the backbone model's weights\n",
    "        self._init_from_backbone(self.patch_embed, getattr(self.backbone_model, 'patch_embed'))\n",
    "        self._init_from_backbone(self.cls_token, getattr(self.backbone_model, 'cls_token'))\n",
    "        self._init_from_backbone(self.pos_embed, getattr(self.backbone_model, 'pos_embed'))\n",
    "        self._init_from_backbone(self.blocks, getattr(self.backbone_model, 'blocks'))\n",
    "        self._init_from_backbone(self.norm, getattr(self.backbone_model, 'fc_norm'))\n",
    "    \n",
    "    def _init_from_backbone(self, target, source):\n",
    "        if isinstance(target, nn.Module):\n",
    "            target.load_state_dict(source.state_dict())\n",
    "        elif isinstance(target, torch.Tensor):\n",
    "            target.data.copy_(source.data)\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for weight initialization\")\n",
    "        \n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "        \n",
    "    def forward(self, x, mask_ratio=0):\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        if mask_ratio > 0:\n",
    "            x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "        \n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_token = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        \n",
    "        # apply Transformer blocks\n",
    "        if mask_ratio > 0:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "            x = self.norm(x)\n",
    "            \n",
    "            return x, mask, ids_restore, None\n",
    "        else:\n",
    "            contextual_features = []\n",
    "            for n, blk in enumerate(self.blocks):\n",
    "                x = blk(x)\n",
    "                if n >= self.contextual_depth:\n",
    "                    contextual_features.append(self.norm(x))\n",
    "            \n",
    "            # -> [N, L=513, D=768]\n",
    "            return torch.stack(contextual_features, dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_size, patch_size, in_chans, num_patches, encoder_dim, decoder_dim, num_layers, num_heads, ff_dim, pos_trainable=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.in_chans = in_chans\n",
    "        self.image_size = image_size if isinstance(image_size, (list, tuple)) else (image_size, image_size)\n",
    "        self.patch_size = patch_size if isinstance(patch_size, (list, tuple)) else (patch_size, patch_size)\n",
    "        self.patch_hw = (self.image_size[0] // self.patch_size[0], self.image_size[1] // self.patch_size[1])\n",
    "        \n",
    "        self.decoder_embed = nn.Linear(encoder_dim, decoder_dim, bias=True)\n",
    "        \n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, decoder_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_dim), requires_grad=pos_trainable)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=decoder_dim, num_heads=num_heads, qkv_bias=True, norm_layer=nn.LayerNorm\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_norm = nn.LayerNorm(decoder_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed_flexible(self.decoder_pos_embed.shape[-1], self.patch_hw, cls_token=True)\n",
    "        \n",
    "        print('decoder_pos_embed', decoder_pos_embed.shape, int(self.num_patches**.5))\n",
    "        print('self.decoder_pos_embed', self.decoder_pos_embed.shape)\n",
    "        \n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            \n",
    "    def forward(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "        \n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        pred = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        return pred, None, None #emb, emb_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_backbone, decoder_config, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.enc_model = ViTEncoder(encoder_backbone)\n",
    "        \n",
    "        decoder_config['encoder_dim'] = self.enc_model.embed_dim\n",
    "        decoder_config['num_patches'] = self.enc_model.num_patches\n",
    "        \n",
    "        self.dec_model = ViTDecoder(**decoder_config)\n",
    "        \n",
    "        self.image_size = self.dec_model.image_size\n",
    "        self.patch_size = self.dec_model.patch_size\n",
    "        self.in_chans = self.dec_model.in_chans\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        L = (H/p)*(W/p)\n",
    "        \"\"\"\n",
    "        \n",
    "        h = imgs.shape[2] // self.patch_size[0]\n",
    "        w = imgs.shape[3] // self.patch_size[1]\n",
    "        \n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 1, h, self.patch_size[0], w, self.patch_size[1]))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, self.patch_size[0]*self.patch_size[1]*self.in_chans))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        specs: (N, 1, H, W)\n",
    "        \"\"\"\n",
    "        h = self.image_size[0] // self.patch_size[0]\n",
    "        w = self.image_size[1] // self.patch_size[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, self.patch_size[0], self.patch_size[1], self.in_chans))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        specs = x.reshape(shape=(x.shape[0], 1, h*self.patch_size[0], w*self.patch_size[1]))\n",
    "        return specs\n",
    "    \n",
    "    def forward_loss(self, imgs, pred, mask, norm_pix_loss=False):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encode_x, mask, ids_restore, _ = self.enc_model(x, self.mask_ratio)\n",
    "        pred, _, _ = self.dec_model(encode_x, ids_restore)\n",
    "        pred = pred[:, 1:, :]\n",
    "        reconstruct_loss = self.forward_loss(x, pred, mask, norm_pix_loss=False)\n",
    "        \n",
    "        return reconstruct_loss, pred, mask, ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_pos_embed (513, 768) 22\n",
      "self.decoder_pos_embed torch.Size([1, 513, 768])\n",
      "tensor(2.4831, grad_fn=<DivBackward0>) torch.Size([1, 512, 256]) torch.Size([1, 512]) torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# test models\n",
    "test_input = torch.randn(1, 1, 1024, 128)\n",
    "\n",
    "# test ViTEncoder\n",
    "backbone_name = \"hf_hub:gaunernst/vit_base_patch16_1024_128.audiomae_as2m_ft_as20k\"\n",
    "enc_model = ViTEncoder(backbone_name)\n",
    "encode_x, mask, ids_restore, _ = enc_model(test_input, 0.75)\n",
    "print(encode_x.shape, mask.shape, ids_restore.shape)\n",
    "\n",
    "# test ViTDecoder\n",
    "decoder_config = dict(\n",
    "    image_size=(1024, 128),\n",
    "    patch_size=16, \n",
    "    in_chans=1, \n",
    "    decoder_dim=768, \n",
    "    num_layers=8, \n",
    "    num_heads=12, \n",
    "    ff_dim=2048\n",
    ")\n",
    "decode_model = ViTDecoder(**decoder_config, encoder_dim=enc_model.embed_dim, num_patches=enc_model.num_patches)\n",
    "pred, _, _ = decode_model(encode_x, ids_restore)\n",
    "print(pred.shape)\n",
    "\n",
    "# test ViTMAE\n",
    "vitmae_model = ViTMAE(backbone_name, decoder_config, mask_ratio=0.75)\n",
    "reconstruct_loss, pred, mask, ids_restore = vitmae_model(test_input)\n",
    "print(reconstruct_loss, pred.shape, mask.shape, ids_restore.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSongDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_audio_meta, config):\n",
    "        self.df_audio_meta = df_audio_meta\n",
    "        self.feature_extractor = ASTFeatureExtractor()\n",
    "        self.config = config\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_audio_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_audio_meta.iloc[idx]\n",
    "        audio_path = f\"{self.config.dataset_dir}/{row['file_name']}\"\n",
    "        audio_arr, sr = self.get_audio(audio_path)\n",
    "        spec = self.feature_extractor(audio_arr, sampling_rate=sr, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return spec['input_values'].squeeze(0), row['species_id']\n",
    "\n",
    "    def get_audio(self, audio_path):\n",
    "        audio, sr = librosa.load(audio_path, sr=self.config.audio_sr)\n",
    "        return audio, sr\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [x[0] for x in batch]\n",
    "    targets = [x[1] for x in batch]\n",
    "    data_dict = {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0),\n",
    "        \"labels\": torch.tensor(targets)\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "# train the BirdASTForPretrain with self-supervised learning\n",
    "\n",
    "bs_dataset = BirdSongDataset(df_audio_meta, config)\n",
    "bs_dataloader = DataLoader(bs_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "backbone_name = \"hf_hub:gaunernst/vit_base_patch16_1024_128.audiomae_as2m_ft_as20k\"\n",
    "\n",
    "decoder_config = dict(\n",
    "    image_size=(1024, 128),\n",
    "    patch_size=16, \n",
    "    in_chans=1, \n",
    "    decoder_dim=768, \n",
    "    num_layers=8, \n",
    "    num_heads=12, \n",
    "    ff_dim=2048\n",
    ")\n",
    "\n",
    "vitmae_model = ViTMAE(backbone_name, decoder_config, mask_ratio=0.75)\n",
    "vitmae_model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(vitmae_model.parameters(), lr=config.max_lr, weight_decay=config.weight_decay)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=config.max_lr, \n",
    "    final_div_factor=config.lr_final_div, \n",
    "    steps_per_epoch=len(bs_dataloader), \n",
    "    epochs=config.epochs\n",
    "    )\n",
    "\n",
    "scaler = GradScaler(enabled=config.amp)\n",
    "\n",
    "loss_records = defaultdict(list)\n",
    "\n",
    "wandb_init(\"BirdAST_Pretrain\", \"BirdAST_Pretrain_Large\", config)\n",
    "logger = get_logger(f\"{config.log_dir}/BirdAST_Pretrain.log\")\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(bs_dataloader), total=len(bs_dataloader)):\n",
    "        \n",
    "        spectrograms = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=config.amp):\n",
    "            loss, pred, mask, ids_restore = vitmae_model(spectrograms)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        loss_meter.update(loss.item())\n",
    "        \n",
    "        wandb.log({\"Loss\": loss.item(), \"Learning Rate\": scheduler.get_last_lr()[0], \"Epoch\": epoch, \"Batch\": i})\n",
    "        \n",
    "        if i % config.print_freq == 0:\n",
    "            logger.info(f\"Epoch: {epoch} | Batch: {i}, Loss: {loss_meter.avg}\")\n",
    "            \n",
    "    wandb.log({\"Epoch_Loss\": loss_meter.avg, \"Epoch\": epoch})\n",
    "    loss_records[f'epoch_{i+1}'].append(loss_meter.avg)\n",
    "    logger.info('-'*10 + \"\\n\" + f\"Epoch: {epoch} | Loss: {loss_meter.avg}\" + \"\\n\" + '-'*10)\n",
    "    \n",
    "    # save model\n",
    "    model_path = f\"{config.log_dir}/ViTMAE_Pretrain_best_{loss_meter.avg:.2f}.pth\"\n",
    "    if loss_meter.avg < best_loss:\n",
    "        best_loss = loss_meter.avg\n",
    "        torch.save(vitmae_model.state_dict(), model_path)\n",
    "        logger.info(f\"Best Model loss = {loss_meter.avg} | saved at: {model_path}\")\n",
    "        \n",
    "    # clear memory\n",
    "    del spectrograms, labels, pred\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
