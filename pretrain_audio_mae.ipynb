{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms-li2022\u001b[0m (\u001b[33msli2024\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import librosa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "from timm.models.vision_transformer import Block\n",
    "from timm.models.layers import to_2tuple\n",
    "\n",
    "from transformers import ASTFeatureExtractor\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Config and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(log_file='log.txt'):\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    # Logging to file\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    # Logging to console\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def wandb_init(project_name, run_name, config):\n",
    "    config_dict = {\n",
    "        k: v for k, v in config.__dict__.items() if not k.startswith('_') and not callable(v) and k != 'copy'\n",
    "    }\n",
    "    run = wandb.init(project=project_name, name=run_name, config=config_dict)\n",
    "    return run\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.value = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.value = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DRIVE_FOLDER = \".\" #\"/content/drive/MyDrive/Colab Notebooks\"\n",
    "KEEP_COLS = ['category_number', 'common_name', 'audio_length', 'type', 'remarks', 'quality', 'scientific_name', 'mp3_link', 'region']\n",
    "\n",
    "class Config:\n",
    "    # path\n",
    "    dataset_dir = f\"{DRIVE_FOLDER}/Audio_GreaterManaus\"  \n",
    "    labels_list = None\n",
    "    model_name = \"Pretrain_ViTMAE_GreaterManaus\"\n",
    "    backbone_name = \"hf_hub:gaunernst/vit_base_patch16_1024_128.audiomae_as2m\" #\"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "    # number of classes in the dataset\n",
    "    n_classes = None \n",
    "    # Audio parameters\n",
    "    audio_sr = 16000 #Hz\n",
    "    segment_length = 10  #s\n",
    "    fft_window = 0.025 #s\n",
    "    hop_window_length = 0.01 #s\n",
    "    n_mels = 128\n",
    "    low_cut = 1000 #Hz\n",
    "    high_cut = 8000 #Hz\n",
    "    top_db = 100\n",
    "    # Training parameters\n",
    "    batch_size =  64 #16\n",
    "    num_workers = 0\n",
    "    n_splits = 5\n",
    "    log_dir = f\"{DRIVE_FOLDER}/training_logs\"\n",
    "    max_lr = 5e-5\n",
    "    epochs = 10\n",
    "    weight_decay = 0.01\n",
    "    lr_final_div = 1000\n",
    "    amp = True\n",
    "    grad_accum_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "    print_epoch_freq = 1\n",
    "    print_freq = 200\n",
    "    # model parameters\n",
    "    n_decoder_layers = 6\n",
    "    n_decoder_heads = 6\n",
    "    ff_dim_decoder = 2048\n",
    "    # seed\n",
    "    random_seed = 2046\n",
    "    \n",
    "    @classmethod\n",
    "    def copy(cls):\n",
    "        new_class = type('CustomConfig', (cls,), {k: v for k, v in cls.__dict__.items() if not k.startswith('__') and not callable(v)})\n",
    "        return new_class\n",
    "    \n",
    "config = Config.copy()\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "seed_everything(config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils for positional embeddings\n",
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_flexible(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size[0], dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size[1], dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size[0], grid_size[1]])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "class PatchEmbed_org(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.patch_hw = (img_size[1] // patch_size[1], img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        #assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #    f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, backbone_name, contextual_depth=8):\n",
    "        super().__init__()\n",
    "        self.backbone_model = timm.create_model(backbone_name, pretrained=True, num_classes=0)\n",
    "        \n",
    "        # desamble the backbone model\n",
    "        self.patch_embed = getattr(self.backbone_model, 'patch_embed')\n",
    "        self.cls_token = getattr(self.backbone_model, 'cls_token')\n",
    "        self.pos_embed = getattr(self.backbone_model, 'pos_embed')\n",
    "        self.blocks = getattr(self.backbone_model, 'blocks')\n",
    "        self.norm = getattr(self.backbone_model, 'fc_norm')\n",
    "        \n",
    "        self.embed_dim = self.backbone_model.embed_dim\n",
    "        self.num_patches = self.patch_embed.num_patches\n",
    "        self.encoder_depth = len(self.blocks)\n",
    "        self.contextual_depth=contextual_depth\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        # Initialize the weights of each module using the backbone model's weights\n",
    "        self._init_from_backbone(self.patch_embed, getattr(self.backbone_model, 'patch_embed'))\n",
    "        self._init_from_backbone(self.cls_token, getattr(self.backbone_model, 'cls_token'))\n",
    "        self._init_from_backbone(self.pos_embed, getattr(self.backbone_model, 'pos_embed'))\n",
    "        self._init_from_backbone(self.blocks, getattr(self.backbone_model, 'blocks'))\n",
    "        self._init_from_backbone(self.norm, getattr(self.backbone_model, 'fc_norm'))\n",
    "    \n",
    "    def _init_from_backbone(self, target, source):\n",
    "        if isinstance(target, nn.Module):\n",
    "            target.load_state_dict(source.state_dict())\n",
    "        elif isinstance(target, torch.Tensor):\n",
    "            target.data.copy_(source.data)\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for weight initialization\")\n",
    "        \n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "        \n",
    "    def forward(self, x, mask_ratio=0):\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        if mask_ratio > 0:\n",
    "            x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "        \n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_token = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        \n",
    "        # apply Transformer blocks\n",
    "        if mask_ratio > 0:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "            x = self.norm(x)\n",
    "            \n",
    "            return x, mask, ids_restore, None\n",
    "        else:\n",
    "            contextual_features = []\n",
    "            for n, blk in enumerate(self.blocks):\n",
    "                x = blk(x)\n",
    "                if n >= self.contextual_depth:\n",
    "                    contextual_features.append(self.norm(x))\n",
    "            \n",
    "            # -> [N, L=513, D=768]\n",
    "            return torch.stack(contextual_features, dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_size, patch_size, in_chans, num_patches, encoder_dim, decoder_dim, num_layers, num_heads, ff_dim, pos_trainable=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.in_chans = in_chans\n",
    "        self.image_size = image_size if isinstance(image_size, (list, tuple)) else (image_size, image_size)\n",
    "        self.patch_size = patch_size if isinstance(patch_size, (list, tuple)) else (patch_size, patch_size)\n",
    "        self.patch_hw = (self.image_size[0] // self.patch_size[0], self.image_size[1] // self.patch_size[1])\n",
    "        \n",
    "        self.decoder_embed = nn.Linear(encoder_dim, decoder_dim, bias=True)\n",
    "        \n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, decoder_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_dim), requires_grad=pos_trainable)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=decoder_dim, num_heads=num_heads, qkv_bias=True, norm_layer=nn.LayerNorm\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_norm = nn.LayerNorm(decoder_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed_flexible(self.decoder_pos_embed.shape[-1], self.patch_hw, cls_token=True)\n",
    "        \n",
    "        print('decoder_pos_embed', decoder_pos_embed.shape, int(self.num_patches**.5))\n",
    "        print('self.decoder_pos_embed', self.decoder_pos_embed.shape)\n",
    "        \n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            \n",
    "    def forward(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "        \n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        pred = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        return pred, None, None #emb, emb_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_backbone, decoder_config, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.enc_model = ViTEncoder(encoder_backbone)\n",
    "        \n",
    "        decoder_config['encoder_dim'] = self.enc_model.embed_dim\n",
    "        decoder_config['num_patches'] = self.enc_model.num_patches\n",
    "        \n",
    "        self.dec_model = ViTDecoder(**decoder_config)\n",
    "        \n",
    "        self.image_size = self.dec_model.image_size\n",
    "        self.patch_size = self.dec_model.patch_size\n",
    "        self.in_chans = self.dec_model.in_chans\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        L = (H/p)*(W/p)\n",
    "        \"\"\"\n",
    "        \n",
    "        h = imgs.shape[2] // self.patch_size[0]\n",
    "        w = imgs.shape[3] // self.patch_size[1]\n",
    "        \n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 1, h, self.patch_size[0], w, self.patch_size[1]))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, self.patch_size[0]*self.patch_size[1]*self.in_chans))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        specs: (N, 1, H, W)\n",
    "        \"\"\"\n",
    "        h = self.image_size[0] // self.patch_size[0]\n",
    "        w = self.image_size[1] // self.patch_size[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, self.patch_size[0], self.patch_size[1], self.in_chans))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        specs = x.reshape(shape=(x.shape[0], 1, h*self.patch_size[0], w*self.patch_size[1]))\n",
    "        return specs\n",
    "    \n",
    "    def forward_loss(self, imgs, pred, mask, norm_pix_loss=False):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encode_x, mask, ids_restore, _ = self.enc_model(x, self.mask_ratio)\n",
    "        pred, _, _ = self.dec_model(encode_x, ids_restore)\n",
    "        pred = pred[:, 1:, :]\n",
    "        reconstruct_loss = self.forward_loss(x, pred, mask, norm_pix_loss=False)\n",
    "        \n",
    "        return reconstruct_loss, pred, mask, ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test models\n",
    "# test_input = torch.randn(1, 1, 1024, 128)\n",
    "\n",
    "# # test ViTEncoder\n",
    "# backbone_name = \"hf_hub:gaunernst/vit_base_patch16_1024_128.audiomae_as2m_ft_as20k\"\n",
    "# enc_model = ViTEncoder(backbone_name)\n",
    "# encode_x, mask, ids_restore, _ = enc_model(test_input, 0.75)\n",
    "# print(encode_x.shape, mask.shape, ids_restore.shape)\n",
    "\n",
    "# # test ViTDecoder\n",
    "# decoder_config = dict(\n",
    "#     image_size=(1024, 128),\n",
    "#     patch_size=16, \n",
    "#     in_chans=1, \n",
    "#     decoder_dim=768, \n",
    "#     num_layers=8, \n",
    "#     num_heads=12, \n",
    "#     ff_dim=2048\n",
    "# )\n",
    "# decode_model = ViTDecoder(**decoder_config, encoder_dim=enc_model.embed_dim, num_patches=enc_model.num_patches)\n",
    "# pred, _, _ = decode_model(encode_x, ids_restore)\n",
    "# print(pred.shape)\n",
    "\n",
    "# # test ViTMAE\n",
    "# vitmae_model = ViTMAE(backbone_name, decoder_config, mask_ratio=0.75)\n",
    "# reconstruct_loss, pred, mask, ids_restore = vitmae_model(test_input)\n",
    "# print(reconstruct_loss, pred.shape, mask.shape, ids_restore.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSongDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_audio_meta, config):\n",
    "        self.df_audio_meta = df_audio_meta\n",
    "        self.feature_extractor = ASTFeatureExtractor()\n",
    "        self.config = config\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_audio_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_audio_meta.iloc[idx]\n",
    "        audio_path = f\"{self.config.dataset_dir}/{row['file_name']}\"\n",
    "        audio_arr, sr = self.get_audio(audio_path)\n",
    "        spec = self.feature_extractor(audio_arr, sampling_rate=sr, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return spec['input_values'].squeeze(0), None\n",
    "\n",
    "    def get_audio(self, audio_path):\n",
    "        audio, sr = librosa.load(audio_path, sr=self.config.audio_sr)\n",
    "        return audio, sr\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [x[0] for x in batch]\n",
    "    targets = [x[1] for x in batch]\n",
    "    data_dict = {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0),\n",
    "        \"labels\":None\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23120, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>sub_location</th>\n",
       "      <th>location_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sampling_rate</th>\n",
       "      <th>mp3_path</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inhaa</td>\n",
       "      <td>Inhaa-Be Audiomoth 2</td>\n",
       "      <td>Inhaa_2</td>\n",
       "      <td>2024-04-05 12:40:00</td>\n",
       "      <td>48000</td>\n",
       "      <td>/content/drive/MyDrive/Colab Notebooks/Audio/I...</td>\n",
       "      <td>data/Inhaa_2_20240405T124000_5.wav</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inhaa</td>\n",
       "      <td>Inhaa-Be Audiomoth 2</td>\n",
       "      <td>Inhaa_2</td>\n",
       "      <td>2024-04-05 12:45:00</td>\n",
       "      <td>48000</td>\n",
       "      <td>/content/drive/MyDrive/Colab Notebooks/Audio/I...</td>\n",
       "      <td>data/Inhaa_2_20240405T124500_39.wav</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inhaa</td>\n",
       "      <td>Inhaa-Be Audiomoth 2</td>\n",
       "      <td>Inhaa_2</td>\n",
       "      <td>2024-04-05 12:50:00</td>\n",
       "      <td>48000</td>\n",
       "      <td>/content/drive/MyDrive/Colab Notebooks/Audio/I...</td>\n",
       "      <td>data/Inhaa_2_20240405T125000_40.wav</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inhaa</td>\n",
       "      <td>Inhaa-Be Audiomoth 2</td>\n",
       "      <td>Inhaa_2</td>\n",
       "      <td>2024-04-05 12:55:00</td>\n",
       "      <td>48000</td>\n",
       "      <td>/content/drive/MyDrive/Colab Notebooks/Audio/I...</td>\n",
       "      <td>data/Inhaa_2_20240405T125500_3.wav</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inhaa</td>\n",
       "      <td>Inhaa-Be Audiomoth 2</td>\n",
       "      <td>Inhaa_2</td>\n",
       "      <td>2024-04-05 13:00:00</td>\n",
       "      <td>48000</td>\n",
       "      <td>/content/drive/MyDrive/Colab Notebooks/Audio/I...</td>\n",
       "      <td>data/Inhaa_2_20240405T130000_25.wav</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location          sub_location location_id            timestamp  \\\n",
       "0    Inhaa  Inhaa-Be Audiomoth 2     Inhaa_2  2024-04-05 12:40:00   \n",
       "1    Inhaa  Inhaa-Be Audiomoth 2     Inhaa_2  2024-04-05 12:45:00   \n",
       "2    Inhaa  Inhaa-Be Audiomoth 2     Inhaa_2  2024-04-05 12:50:00   \n",
       "3    Inhaa  Inhaa-Be Audiomoth 2     Inhaa_2  2024-04-05 12:55:00   \n",
       "4    Inhaa  Inhaa-Be Audiomoth 2     Inhaa_2  2024-04-05 13:00:00   \n",
       "\n",
       "   sampling_rate                                           mp3_path  \\\n",
       "0          48000  /content/drive/MyDrive/Colab Notebooks/Audio/I...   \n",
       "1          48000  /content/drive/MyDrive/Colab Notebooks/Audio/I...   \n",
       "2          48000  /content/drive/MyDrive/Colab Notebooks/Audio/I...   \n",
       "3          48000  /content/drive/MyDrive/Colab Notebooks/Audio/I...   \n",
       "4          48000  /content/drive/MyDrive/Colab Notebooks/Audio/I...   \n",
       "\n",
       "                             file_name  file_exists  \n",
       "0   data/Inhaa_2_20240405T124000_5.wav         True  \n",
       "1  data/Inhaa_2_20240405T124500_39.wav         True  \n",
       "2  data/Inhaa_2_20240405T125000_40.wav         True  \n",
       "3   data/Inhaa_2_20240405T125500_3.wav         True  \n",
       "4  data/Inhaa_2_20240405T130000_25.wav         True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_audios = pd.read_csv(f\"{config.dataset_dir}/metadata.csv\")\n",
    "\n",
    "\n",
    "df_audios = df_audios[~df_audios.isnull().any(axis=1)]\n",
    "df_audios = df_audios[df_audios['file_name'].str.contains(\"wav\")]\n",
    "df_audios['file_exists'] = df_audios['file_name'].apply(lambda x: os.path.exists(f\"{config.dataset_dir}/{x}\"))\n",
    "\n",
    "df_audios = df_audios[df_audios['file_exists']].copy().reset_index(drop=True)\n",
    "\n",
    "# index 8044 is broken\n",
    "df_audios = df_audios.drop(8044)\n",
    "df_audios = df_audios.reset_index(drop=True)\n",
    "\n",
    "print(df_audios.shape)\n",
    "\n",
    "df_audios.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from Hugging Face hub (gaunernst/vit_base_patch16_1024_128.audiomae_as2m)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# train the BirdASTForPretrain with self-supervised learning\n",
    "\n",
    "bs_dataset = BirdSongDataset(df_audios, config)\n",
    "bs_dataloader = DataLoader(bs_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "backbone_name = config.backbone_name\n",
    "\n",
    "decoder_config = dict(\n",
    "    image_size=(1024, 128),\n",
    "    patch_size=16, \n",
    "    in_chans=1, \n",
    "    decoder_dim=768, \n",
    "    num_layers=8, \n",
    "    num_heads=12, \n",
    "    ff_dim=2048\n",
    ")\n",
    "\n",
    "vitmae_model = ViTMAE(backbone_name, decoder_config, mask_ratio=0.75)\n",
    "vitmae_model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(vitmae_model.parameters(), lr=config.max_lr, weight_decay=config.weight_decay)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=config.max_lr, \n",
    "    final_div_factor=config.lr_final_div, \n",
    "    steps_per_epoch=len(bs_dataloader), \n",
    "    epochs=config.epochs\n",
    "    )\n",
    "\n",
    "scaler = GradScaler(enabled=config.amp)\n",
    "\n",
    "loss_records = defaultdict(list)\n",
    "\n",
    "wandb_init(config.model_name, config.model_name, config)\n",
    "logger = get_logger(f\"{config.log_dir}/{config.model_name}_pre-training.log\")\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(bs_dataloader), total=len(bs_dataloader)):\n",
    "        \n",
    "        spectrograms = batch['input_ids'].to(DEVICE)\n",
    "        # labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=config.amp):\n",
    "            loss, pred, mask, ids_restore = vitmae_model(spectrograms.unsqueeze(1))\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        loss_meter.update(loss.item())\n",
    "        \n",
    "        wandb.log({\"Loss\": loss.item(), \"Learning Rate\": scheduler.get_last_lr()[0], \"Epoch\": epoch, \"Batch\": i})\n",
    "        \n",
    "        if i % config.print_freq == 0:\n",
    "            logger.info(f\"Epoch: {epoch} | Batch: {i}, Loss: {loss_meter.avg}\")\n",
    "            \n",
    "    wandb.log({\"Epoch_Loss\": loss_meter.avg, \"Epoch\": epoch})\n",
    "    loss_records[f'epoch_{i+1}'].append(loss_meter.avg)\n",
    "    logger.info('-'*10 + \"\\n\" + f\"Epoch: {epoch} | Loss: {loss_meter.avg}\" + \"\\n\" + '-'*10)\n",
    "    \n",
    "    # save model\n",
    "    model_path = f\"{config.log_dir}/ViTMAE_Pretrain_best_{loss_meter.avg:.2f}.pth\"\n",
    "    if loss_meter.avg < best_loss:\n",
    "        best_loss = loss_meter.avg\n",
    "        torch.save(vitmae_model.state_dict(), model_path)\n",
    "        logger.info(f\"Best Model loss = {loss_meter.avg} | saved at: {model_path}\")\n",
    "        \n",
    "    # clear memory\n",
    "    del spectrograms, pred\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view model results\n",
    "\n",
    "backbone_name = config.backbone_name\n",
    "\n",
    "decoder_config = dict(\n",
    "    image_size=(1024, 128),\n",
    "    patch_size=16, \n",
    "    in_chans=1, \n",
    "    decoder_dim=768, \n",
    "    num_layers=8, \n",
    "    num_heads=12, \n",
    "    ff_dim=2048\n",
    ")\n",
    "\n",
    "vitmae_model = ViTMAE(backbone_name, decoder_config, mask_ratio=0.75)\n",
    "\n",
    "model_weights = \"./training_logs/ViTMAE_Pretrain_best_0.47.pth\"\n",
    "\n",
    "# load weights\n",
    "vitmae_model.load_state_dict(torch.load(model_weights, map_location=DEVICE))\n",
    "vitmae_model.to(DEVICE)\n",
    "\n",
    "# test the model\n",
    "\n",
    "for i, batch in tqdm(enumerate(bs_dataloader), total=len(bs_dataloader)):\n",
    "    \n",
    "    spectrograms = batch['input_ids'].to(DEVICE)\n",
    "    labels = batch['labels'].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstruct_loss, pred, mask, ids_restore = vitmae_model(spectrograms.unsqueeze(1))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_patches = vitmae_model.patchify(spectrograms.unsqueeze(1))\n",
    "\n",
    "# apply mask to the patches, use zero for removed patches\n",
    "spec_patches[mask == 1] = 0\n",
    "\n",
    "specs_masked = vitmae_model.unpatchify(spec_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = vitmae_model.unpatchify(pred) # [N, 1, H, W]\n",
    "\n",
    "spec_id = 12\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True, sharey=True)\n",
    "imshow_kwargs = dict(aspect='auto', cmap='plasma', vmax=1, vmin=-1, origin='lower')\n",
    "\n",
    "ax = axes[0]\n",
    "ax.imshow(spectrograms[spec_id].detach().cpu().numpy().T, **imshow_kwargs)\n",
    "ax.set_title(f\"Original Spectrogram\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.imshow(specs_masked[spec_id].detach().cpu().numpy().squeeze(0).T, **imshow_kwargs)\n",
    "ax.set_title(f\"Masked Spectrogram\")\n",
    "\n",
    "ax = axes[2]\n",
    "ax.imshow(reconstructed[spec_id].detach().cpu().numpy().squeeze(0).T, **imshow_kwargs)\n",
    "ax.set_title(f\"Reconstructed Spectrogram\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
