{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchaudio.compliance import kaldi\n",
    "import torch.nn.functional as F\n",
    "import json, requests, timm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdAudioMAEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df_audio_meta, dataset_dir, sampling_rate=16_000):\n",
    "        self.df_audio_meta = df_audio_meta\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.global_mean = -4.2677393\n",
    "        self.global_std = 4.5689974\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_audio_meta)\n",
    "\n",
    "    def preprocess(self, x: torch.Tensor):\n",
    "        x = x - x.mean()\n",
    "        melspec = kaldi.fbank(x.unsqueeze(0), htk_compat=True, window_type=\"hanning\", num_mel_bins=128)\n",
    "        if melspec.shape[0] < 1024:\n",
    "            melspec = F.pad(melspec, (0, 0, 0, 1024 - melspec.shape[0]))\n",
    "        else:\n",
    "            random_start = np.random.randint(0, len(melspec)-1025)\n",
    "            melspec = melspec[random_start:random_start+1024]\n",
    "        melspec = (melspec - self.global_mean) / (self.global_std * 2)\n",
    "        return melspec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_audio_meta.iloc[idx]\n",
    "        audio_path = f\"{self.dataset_dir}/{row['file_name']}\"\n",
    "        # load audio file\n",
    "        audio_arr, sr = librosa.load(audio_path, sr=self.sampling_rate)\n",
    "        # get the spectrogram\n",
    "        spec = self.preprocess(torch.tensor(audio_arr))\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = \"gaunernst/vit_base_patch16_1024_128.audiomae_as2m_ft_as20k\"\n",
    "MODEL = timm.create_model(f\"hf_hub:{TAG}\", pretrained=True)\n",
    "\n",
    "LABEL_URL = \"https://huggingface.co/datasets/huggingface/label-files/raw/main/audioset-id2label.json\"\n",
    "AUDIOSET_LABELS = list(json.loads(requests.get(LABEL_URL).content).values())\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BirdAudioMAEDataset(df_audios, f\"{DRIVE_FOLDER}/Audio_GreaterManaus\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, num_workers=2)\n",
    "\n",
    "probs_collect, classes_collect = [], []\n",
    "\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()\n",
    "\n",
    "# ~ 3.5 hours\n",
    "for i, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "    # if i == 0:\n",
    "    #     print(\"Example Spectrogram:\")\n",
    "    #     plt.figure(figsize=(9, 2))\n",
    "    #     plt.imshow(data[0].numpy().T, aspect='auto', origin='lower', cmap='cool')\n",
    "    #     plt.colorbar()\n",
    "    #     plt.show()\n",
    "\n",
    "    data = data.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = MODEL(data.unsqueeze(1))\n",
    "        topk_probs, topk_classes = logits.sigmoid().topk(10)\n",
    "\n",
    "    probs_collect.append(topk_probs.cpu().numpy())\n",
    "    classes_collect.append(topk_classes.cpu().numpy())\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Finished {i} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate all results\n",
    "all_probs = np.concatenate(probs_collect)\n",
    "all_classes = np.concatenate(classes_collect)\n",
    "\n",
    "class_cols = [f'class_{i+1}' for i in range(10)]\n",
    "proba_cols = [f'probability_{i+1}' for i in range(10)]\n",
    "\n",
    "df_results = pd.DataFrame(\n",
    "    np.concatenate([all_classes, all_probs], axis=1),\n",
    "    columns = class_cols + proba_cols\n",
    ")\n",
    "\n",
    "df_results[class_cols] = df_results[class_cols].astype(int)\n",
    "df_results[proba_cols] *= 100\n",
    "\n",
    "id_to_label = {i: v for i, v in enumerate(AUDIOSET_LABELS)}\n",
    "for _col in class_cols:\n",
    "    df_results[_col] = df_results[_col].map(id_to_label)\n",
    "\n",
    "df_audios_labels = df_audios.join(df_results, how='left').dropna()\n",
    "\n",
    "df_audios_labels.to_csv(f\"{DRIVE_FOLDER}/Audio_GreaterManaus/Audio_Segment_Labels.csv\",index=False)\n",
    "\n",
    "df_audios_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
