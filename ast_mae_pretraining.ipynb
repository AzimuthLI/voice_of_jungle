{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import librosa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import ASTConfig, ASTFeatureExtractor, ASTModel\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from time import time\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(log_file='log.txt'):\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    # Logging to file\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    # Logging to console\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def wandb_init(project_name, run_name, config):\n",
    "    config_dict = {\n",
    "        k: v for k, v in config.__dict__.items() if not k.startswith('_') and not callable(v) and k != 'copy'\n",
    "    }\n",
    "    run = wandb.init(project=project_name, name=run_name, config=config_dict)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_FOLDER = \".\" #\"/content/drive/MyDrive/Colab Notebooks\"\n",
    "KEEP_COLS = ['category_number', 'common_name', 'audio_length', 'type', 'remarks', 'quality', 'scientific_name', 'mp3_link', 'region']\n",
    "\n",
    "class Config:\n",
    "    # path\n",
    "    dataset_dir = f\"{DRIVE_FOLDER}/Audio_XenoCanto\"\n",
    "    labels_list = f\"{DRIVE_FOLDER}/xeno_labels.csv\"\n",
    "    model_name = \"ast_baseline\"\n",
    "    backbone_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "    # number of classes in the dataset\n",
    "    n_classes = 397 \n",
    "    # Audio parameters\n",
    "    audio_sr = 16000 #Hz\n",
    "    segment_length = 10  #s\n",
    "    fft_window = 0.025 #s\n",
    "    hop_window_length = 0.01 #s\n",
    "    n_mels = 128\n",
    "    low_cut = 1000 #Hz\n",
    "    high_cut = 8000 #Hz\n",
    "    top_db = 100\n",
    "    # Training parameters\n",
    "    batch_size = 4 \n",
    "    num_workers = 0\n",
    "    n_splits = 5\n",
    "    log_dir = f\"{DRIVE_FOLDER}/training_logs\"\n",
    "    max_lr = 1e-5\n",
    "    epochs = 10\n",
    "    weight_decay = 0.01\n",
    "    lr_final_div = 1000\n",
    "    amp = True\n",
    "    grad_accum_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "    print_epoch_freq = 1\n",
    "    print_freq = 200\n",
    "    # model parameters\n",
    "    n_decoder_layers = 6\n",
    "    n_decoder_heads = 6\n",
    "    ff_dim_decoder = 2048\n",
    "    # seed\n",
    "    random_seed = 2046\n",
    "    \n",
    "    @classmethod\n",
    "    def copy(cls):\n",
    "        new_class = type('CustomConfig', (cls,), {k: v for k, v in cls.__dict__.items() if not k.startswith('__') and not callable(v)})\n",
    "        return new_class\n",
    "    \n",
    "config = Config.copy()\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "seed_everything(config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audio_meta = pd.read_csv(f\"{config.dataset_dir}/metadata.csv\", nrows=None)\n",
    "df_audio_meta = df_audio_meta.dropna().reset_index(drop=True)\n",
    "\n",
    "# Filter out files that do not exist\n",
    "df_audio_meta['file_exists'] = df_audio_meta['file_name'].apply(lambda x: os.path.exists(f\"{config.dataset_dir}/{x}\"))\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['file_exists']].reset_index(drop=True)\n",
    "\n",
    "# parse scientific names\n",
    "df_audio_meta['scientific_name'] = df_audio_meta['scientific_name'].apply(lambda x: \"_\".join(x.split(\" \")))\n",
    "\n",
    "# drop species with less than 2 samples\n",
    "class_counts = df_audio_meta['scientific_name'].value_counts()\n",
    "print(f\"Number of classes with less than 2 samples: {len(class_counts[class_counts < 2])}\")\n",
    "\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['scientific_name'].isin(class_counts[class_counts > 1].index)].copy().reset_index(drop=True)\n",
    "\n",
    "# encode scientific names to label ids\n",
    "label_ids_list = df_audio_meta['scientific_name'].unique().tolist()\n",
    "label_ids_list.sort()\n",
    "label_to_id = {label: i for i, label in enumerate(label_ids_list)}\n",
    "df_audio_meta['species_id'] = df_audio_meta['scientific_name'].map(label_to_id)\n",
    "\n",
    "# drop samples with no labels\n",
    "df_audio_meta.dropna(subset=['species_id'], inplace=True)\n",
    "df_audio_meta.reset_index(drop=True, inplace=True)\n",
    "df_audio_meta['species_id'] = df_audio_meta['species_id'].astype(int)\n",
    "\n",
    "print(f\"Number of classes in dataset: {df_audio_meta['species_id'].nunique()}\")\n",
    "print(f'Number of samples:', len(df_audio_meta))\n",
    "\n",
    "# save the number of classes in the config\n",
    "config.n_classes = df_audio_meta['species_id'].nunique()\n",
    "\n",
    "df_audio_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDecoder(nn.Module):\n",
    "    def __init__(self, enc_dim, d_dim=768, n_layers=2, n_heads=6, ff_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu', \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.decoder_embed = nn.Linear(enc_dim, d_dim, bias=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, d_dim)).to(DEVICE)\n",
    "        \n",
    "        # random initialization of position embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, d_dim)).to(DEVICE)\n",
    "        \n",
    "    def forward(self, tgt, memory, mask_indices):\n",
    "        \n",
    "        # tgt: (batch_size, seq_len, enc_dim) sequence of image patches\n",
    "        # memory: (batch_size, seq_len, enc_dim) embeddings from encoder\n",
    "        # position_embedding: (seq_len, enc_dim) positional embedding for each patch\n",
    "        # mask_indices: (batch_size, n_mask_indices) indices of patches to mask\n",
    "        \n",
    "        batch_size, seq_len, d_dim = tgt.size()\n",
    "        \n",
    "        # Embed and add position embedding\n",
    "        pos_embedding = self.pos_embedding.expand(batch_size, seq_len, -1)\n",
    "        tgt = self.decoder_embed(tgt) + pos_embedding\n",
    "        \n",
    "        # Mask tokens\n",
    "        mask_tokens = self.mask_token.repeat(batch_size, mask_indices.size(1), 1)\n",
    "        tgt.scatter_(1, mask_indices.unsqueeze(-1).expand(-1, -1, d_dim), mask_tokens)\n",
    "        memory.scatter_(1, mask_indices.unsqueeze(-1).expand(-1, -1, d_dim), mask_tokens)\n",
    "        \n",
    "        tgt_key_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool, device=tgt.device)\n",
    "        tgt_key_padding_mask.scatter_(1, mask_indices, True)\n",
    "        \n",
    "        mem_key_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool, device=tgt.device)\n",
    "        mem_key_padding_mask.scatter_(1, mask_indices, True)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_tokens = self.decoder(\n",
    "            tgt, \n",
    "            memory, \n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=mem_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return decoded_tokens\n",
    "    \n",
    "    \n",
    "class BirdEncoder(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        backbone_config = ASTConfig.from_pretrained(backbone_name)\n",
    "        self.encoder_model = ASTModel.from_pretrained(backbone_name, config=backbone_config)\n",
    "        self.hidden_size = self.encoder_model.config.hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch_size, t_len, f_len) input spectrograms\n",
    "        # output: (batch_size, seq_len, enc_dim) embeddings\n",
    "        \n",
    "        spec_embeddings = self.encoder_model.embeddings(x)\n",
    "        encoder_outputs = self.encoder_model.encoder(spec_embeddings)\n",
    "        \n",
    "        return encoder_outputs.last_hidden_state, spec_embeddings\n",
    "    \n",
    "    \n",
    "class BirdASTForPretrain(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone_name, mask_ratio=0.75, d_dim=768, n_layers=2, n_heads=6, ff_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = BirdEncoder(backbone_name)\n",
    "        \n",
    "        enc_dim = self.encoder.hidden_size\n",
    "        self.decoder = BirdDecoder(enc_dim, d_dim, n_layers, n_heads, ff_dim, dropout)\n",
    "        \n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, t_len, f_len) input spectrograms\n",
    "        # output: (batch_size, t_len, f_len) reconstructed spectrograms\n",
    "        \n",
    "        batch_size, t_len, f_len = x.size()\n",
    "        memory, spec_embeddings = self.encoder(x)\n",
    "        \n",
    "        # Mask indices\n",
    "        seq_len = memory.size(1)\n",
    "        mask_indices = torch.randint(0, seq_len, (batch_size, int(self.mask_ratio * seq_len)), device=x.device)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_tokens = self.decoder(spec_embeddings, memory, mask_indices)\n",
    "        \n",
    "        return decoded_tokens \n",
    "    \n",
    "\n",
    "def get_shape(config):\n",
    "    # see Karpathy's cs231n blog on how to calculate the output dimensions\n",
    "    # https://cs231n.github.io/convolutional-networks/#conv\n",
    "    frequency_out_dimension = (config.num_mel_bins - config.patch_size) // config.frequency_stride + 1\n",
    "    time_out_dimension = (config.max_length - config.patch_size) // config.time_stride + 1\n",
    "\n",
    "    return frequency_out_dimension, time_out_dimension\n",
    "\n",
    "\n",
    "def patchify_spectrogram(spectrograms, config):\n",
    "    # spectrograms: (batch_size, max_length, num_mel_bins)\n",
    "    \n",
    "    batch_size = spectrograms.size(0)\n",
    "    patch_size = config.patch_size\n",
    "    time_stride = config.time_stride\n",
    "    frequency_stride = config.frequency_stride\n",
    "\n",
    "    patches = spectrograms.unfold(1, patch_size, time_stride).unfold(2, patch_size, frequency_stride)\n",
    "    patches = patches.unsqueeze(3).expand(-1, -1, -1, 3, -1, -1)\n",
    "\n",
    "    num_patches_y = patches.size(1)\n",
    "    num_patches_x = patches.size(2)\n",
    "\n",
    "    flattened_patches = patches.contiguous().view(batch_size, num_patches_y * num_patches_x, -1)\n",
    "\n",
    "    # -> (batch_size, num_patches_y * num_patches_x, 3*patch_size*patch_size)\n",
    "    return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSongDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_audio_meta, config):\n",
    "        self.df_audio_meta = df_audio_meta\n",
    "        self.feature_extractor = ASTFeatureExtractor()\n",
    "        self.config = config\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_audio_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_audio_meta.iloc[idx]\n",
    "        audio_path = f\"{self.config.dataset_dir}/{row['file_name']}\"\n",
    "        audio_arr, sr = self.get_audio(audio_path)\n",
    "        spec = self.feature_extractor(audio_arr, sampling_rate=sr, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return spec['input_values'].squeeze(0), row['species_id']\n",
    "\n",
    "    def get_audio(self, audio_path):\n",
    "        audio, sr = librosa.load(audio_path, sr=self.config.audio_sr)\n",
    "        return audio, sr\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [x[0] for x in batch]\n",
    "    targets = [x[1] for x in batch]\n",
    "    data_dict = {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0),\n",
    "        \"labels\": torch.tensor(targets)\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the dataset, dataloader and patchify function\n",
    "# backbone_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "# backbone_config = ASTConfig.from_pretrained(backbone_name)\n",
    "\n",
    "# bs_dataset = BirdSongDataset(df_audio_meta, config)\n",
    "# bs_dataloader = DataLoader(bs_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# for batch in bs_dataloader:\n",
    "#     spectrograms = batch['input_ids']\n",
    "#     labels = batch['labels']\n",
    "#     break\n",
    "\n",
    "# spectrogram_patches = patchify_spectrogram(spectrograms, backbone_config)\n",
    "\n",
    "# print(spectrograms.size())\n",
    "# print(spectrogram_patches.size())\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(6, 12))\n",
    "# ax.imshow(spectrograms[0].cpu().numpy(), aspect='auto', vmax=1, vmin=-1)\n",
    "# ax.set_title(\"Spectrogram\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot patches\n",
    "# fig, axes = plt.subplots(time_dim, freq_dim, figsize=(freq_dim * 2, time_dim * 2))\n",
    "# patch_size = 16\n",
    "# for i in range(time_dim):\n",
    "#     for j in range(freq_dim):\n",
    "#         patch_index = i * freq_dim + j\n",
    "#         patch = spectrogram_patches[0, patch_index].view(3, patch_size, patch_size).permute(1, 2, 0).numpy()\n",
    "#         ax = axes[i, j]\n",
    "#         ax.imshow(patch[:, :, 0], aspect='auto',  vmax=1, vmin=-1)\n",
    "#         ax.set_title(f'Patch {patch_index + 1}')\n",
    "#         ax.axis('off') \n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.value = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.value = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "# train the BirdASTForPretrain with self-supervised learning\n",
    "\n",
    "backbone_config = ASTConfig.from_pretrained(config.backbone_name)\n",
    "\n",
    "bs_dataset = BirdSongDataset(df_audio_meta, config)\n",
    "bs_dataloader = DataLoader(bs_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = BirdASTForPretrain(\n",
    "    config.backbone_name, \n",
    "    mask_ratio=0.75, \n",
    "    d_dim=768, \n",
    "    n_layers=config.n_decoder_layers, \n",
    "    n_heads=config.n_decoder_heads, \n",
    "    ff_dim=config.ff_dim_decoder, \n",
    "    dropout=0.0\n",
    "    )\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config.max_lr, weight_decay=config.weight_decay)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=config.max_lr, \n",
    "    final_div_factor=config.lr_final_div, \n",
    "    steps_per_epoch=len(bs_dataloader), \n",
    "    epochs=config.epochs\n",
    "    )\n",
    "\n",
    "scaler = GradScaler(enabled=config.amp)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "loss_records = defaultdict(list)\n",
    "\n",
    "wandb_init(\"BirdAST_Pretrain\", \"BirdAST_Pretrain_Large\", config)\n",
    "logger = get_logger(f\"{config.log_dir}/BirdAST_Pretrain.log\")\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(bs_dataloader), total=len(bs_dataloader)):\n",
    "        \n",
    "        spectrograms = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=config.amp):\n",
    "            reconstructed_spectrograms = model(spectrograms)\n",
    "            spectrogram_patches = patchify_spectrogram(spectrograms, backbone_config)\n",
    "            loss = loss_fn(reconstructed_spectrograms[:, 2:, :], spectrogram_patches)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        loss_meter.update(loss.item())\n",
    "        \n",
    "        wandb.log({\"Loss\": loss.item(), \"Learning Rate\": scheduler.get_last_lr()[0], \"Epoch\": epoch, \"Batch\": i})\n",
    "        \n",
    "        if i % config.print_freq == 0:\n",
    "            logger.info(f\"Epoch: {epoch} | Batch: {i}, Loss: {loss_meter.avg}\")\n",
    "            \n",
    "    wandb.log({\"Epoch_Loss\": loss_meter.avg, \"Epoch\": epoch})\n",
    "    loss_records[f'epoch_{i+1}'].append(loss_meter.avg)\n",
    "    logger.info('-'*10 + \"\\n\" + f\"Epoch: {epoch} | Loss: {loss_meter.avg}\" + \"\\n\" + '-'*10)\n",
    "    \n",
    "    # save model\n",
    "    model_path = f\"{config.log_dir}/BirdAST_Pretrain_epoch_{epoch}.pth\"\n",
    "    if loss_meter.avg < best_loss:\n",
    "        best_loss = loss_meter.avg\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        logger.info(f\"Best Model loss = {loss_meter.avg} | saved at: {model_path}\")\n",
    "        \n",
    "    # clear memory\n",
    "    del spectrograms, labels, reconstructed_spectrograms, spectrogram_patches \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in tqdm(enumerate(bs_dataloader), total=len(bs_dataloader)):\n",
    "        \n",
    "    spectrograms = batch['input_ids'].to(DEVICE)\n",
    "    labels = batch['labels'].to(DEVICE)\n",
    "    \n",
    "    reconstructed_spectrograms = model(spectrograms)\n",
    "        \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_patches = patchify_spectrogram(spectrograms, backbone_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpathify(spec_patches, time_dim=101, freq_dim=12, patch_size=16, n_channels=3):\n",
    "    # spec_patches: (batch_size, num_patches, patch_size*patch_size*3\n",
    "    \n",
    "    batch_size, num_patches, d_dim = spec_patches.size()\n",
    "    \n",
    "    patches = spec_patches.view(batch_size, time_dim, freq_dim, n_channels, patch_size, patch_size)\n",
    "   \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_unpatched = unpathify(spectrogram_patches)\n",
    "\n",
    "spec_unpatched.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot patches\n",
    "\n",
    "time_dim = 101\n",
    "freq_dim = 12\n",
    "\n",
    "fig, axes = plt.subplots(time_dim, freq_dim, figsize=(freq_dim * 2, time_dim * 2))\n",
    "patch_size = 16\n",
    "for i in range(time_dim):\n",
    "    for j in range(freq_dim):\n",
    "        patch = spec_unpatched[0, i, j].cpu().numpy()\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(patch[0, :, :], aspect='auto',  vmax=1, vmin=-1)\n",
    "        ax.set_title(f'Patch {i}, {j}')\n",
    "        ax.axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_specs_unpatched = unpathify(reconstructed_spectrograms[:, 2:, :])\n",
    "\n",
    "print(reconstructed_specs_unpatched.size())\n",
    "\n",
    "fig, axes = plt.subplots(time_dim, freq_dim, figsize=(freq_dim * 2, time_dim * 2))\n",
    "patch_size = 16\n",
    "for i in range(time_dim):\n",
    "    for j in range(freq_dim):\n",
    "        patch = reconstructed_specs_unpatched[0, i, j].detach().cpu().numpy()\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(patch[0, :, :], aspect='auto',  vmax=1, vmin=-1)\n",
    "        ax.set_title(f'Patch {i}, {j}')\n",
    "        ax.axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
