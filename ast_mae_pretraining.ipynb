{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import librosa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import ASTConfig, ASTFeatureExtractor, ASTModel\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from time import time\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(log_file='log.txt'):\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    # Logging to file\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    # Logging to console\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def wandb_init(project_name, run_name, config):\n",
    "    config_dict = {\n",
    "        k: v for k, v in config.__dict__.items() if not k.startswith('_') and not callable(v) and k != 'copy'\n",
    "    }\n",
    "    run = wandb.init(project=project_name, name=run_name, config=config_dict)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_FOLDER = \".\" #\"/content/drive/MyDrive/Colab Notebooks\"\n",
    "KEEP_COLS = ['category_number', 'common_name', 'audio_length', 'type', 'remarks', 'quality', 'scientific_name', 'mp3_link', 'region']\n",
    "\n",
    "class Config:\n",
    "    # path\n",
    "    dataset_dir = f\"{DRIVE_FOLDER}/Audio_XenoCanto\"\n",
    "    labels_list = f\"{DRIVE_FOLDER}/xeno_labels.csv\"\n",
    "    model_name = \"ast_baseline\"\n",
    "    backbone_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "    # number of classes in the dataset\n",
    "    n_classes = 397 \n",
    "    # Audio parameters\n",
    "    audio_sr = 16000 #Hz\n",
    "    segment_length = 10  #s\n",
    "    fft_window = 0.025 #s\n",
    "    hop_window_length = 0.01 #s\n",
    "    n_mels = 128\n",
    "    low_cut = 1000 #Hz\n",
    "    high_cut = 8000 #Hz\n",
    "    top_db = 100\n",
    "    # Training parameters\n",
    "    batch_size = 4 \n",
    "    num_workers = 0\n",
    "    n_splits = 5\n",
    "    log_dir = f\"{DRIVE_FOLDER}/training_logs\"\n",
    "    max_lr = 1e-5\n",
    "    epochs = 10\n",
    "    weight_decay = 0.01\n",
    "    lr_final_div = 1000\n",
    "    amp = True\n",
    "    grad_accum_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "    print_epoch_freq = 1\n",
    "    print_freq = 200\n",
    "    # model parameters\n",
    "    n_decoder_layers = 6\n",
    "    n_decoder_heads = 6\n",
    "    ff_dim_decoder = 2048\n",
    "    # seed\n",
    "    random_seed = 2046\n",
    "    \n",
    "    @classmethod\n",
    "    def copy(cls):\n",
    "        new_class = type('CustomConfig', (cls,), {k: v for k, v in cls.__dict__.items() if not k.startswith('__') and not callable(v)})\n",
    "        return new_class\n",
    "    \n",
    "config = Config.copy()\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "seed_everything(config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audio_meta = pd.read_csv(f\"{config.dataset_dir}/metadata.csv\", nrows=None)\n",
    "df_audio_meta = df_audio_meta.dropna().reset_index(drop=True)\n",
    "\n",
    "# Filter out files that do not exist\n",
    "df_audio_meta['file_exists'] = df_audio_meta['file_name'].apply(lambda x: os.path.exists(f\"{config.dataset_dir}/{x}\"))\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['file_exists']].reset_index(drop=True)\n",
    "\n",
    "# parse scientific names\n",
    "df_audio_meta['scientific_name'] = df_audio_meta['scientific_name'].apply(lambda x: \"_\".join(x.split(\" \")))\n",
    "\n",
    "# drop species with less than 2 samples\n",
    "class_counts = df_audio_meta['scientific_name'].value_counts()\n",
    "print(f\"Number of classes with less than 2 samples: {len(class_counts[class_counts < 2])}\")\n",
    "\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['scientific_name'].isin(class_counts[class_counts > 1].index)].copy().reset_index(drop=True)\n",
    "\n",
    "# encode scientific names to label ids\n",
    "label_ids_list = df_audio_meta['scientific_name'].unique().tolist()\n",
    "label_ids_list.sort()\n",
    "label_to_id = {label: i for i, label in enumerate(label_ids_list)}\n",
    "df_audio_meta['species_id'] = df_audio_meta['scientific_name'].map(label_to_id)\n",
    "\n",
    "# drop samples with no labels\n",
    "df_audio_meta.dropna(subset=['species_id'], inplace=True)\n",
    "df_audio_meta.reset_index(drop=True, inplace=True)\n",
    "df_audio_meta['species_id'] = df_audio_meta['species_id'].astype(int)\n",
    "\n",
    "print(f\"Number of classes in dataset: {df_audio_meta['species_id'].nunique()}\")\n",
    "print(f'Number of samples:', len(df_audio_meta))\n",
    "\n",
    "# save the number of classes in the config\n",
    "config.n_classes = df_audio_meta['species_id'].nunique()\n",
    "\n",
    "df_audio_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDecoder(nn.Module):\n",
    "    def __init__(self, enc_dim, d_dim=768, n_layers=2, n_heads=6, ff_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu', \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.decoder_embed = nn.Linear(enc_dim, d_dim, bias=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, d_dim)).to(DEVICE)\n",
    "        \n",
    "        # random initialization of position embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, d_dim)).to(DEVICE)\n",
    "        \n",
    "    def forward(self, tgt, memory, mask_indices):\n",
    "        \n",
    "        # tgt: (batch_size, seq_len, enc_dim) sequence of image patches\n",
    "        # memory: (batch_size, seq_len, enc_dim) embeddings from encoder\n",
    "        # position_embedding: (seq_len, enc_dim) positional embedding for each patch\n",
    "        # mask_indices: (batch_size, n_mask_indices) indices of patches to mask\n",
    "        \n",
    "        batch_size, seq_len, d_dim = tgt.size()\n",
    "        \n",
    "        # Embed and add position embedding\n",
    "        pos_embedding = self.pos_embedding.expand(batch_size, seq_len, -1)\n",
    "        tgt = self.decoder_embed(tgt) + pos_embedding\n",
    "        \n",
    "        # Mask tokens\n",
    "        mask_tokens = self.mask_token.repeat(batch_size, mask_indices.size(1), 1)\n",
    "        tgt.scatter_(1, mask_indices.unsqueeze(-1).expand(-1, -1, d_dim), mask_tokens)\n",
    "        memory.scatter_(1, mask_indices.unsqueeze(-1).expand(-1, -1, d_dim), mask_tokens)\n",
    "        \n",
    "        tgt_key_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool, device=tgt.device)\n",
    "        tgt_key_padding_mask.scatter_(1, mask_indices, True)\n",
    "        \n",
    "        mem_key_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool, device=tgt.device)\n",
    "        mem_key_padding_mask.scatter_(1, mask_indices, True)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_tokens = self.decoder(\n",
    "            tgt, \n",
    "            memory, \n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=mem_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return decoded_tokens\n",
    "    \n",
    "    \n",
    "class BirdEncoder(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        backbone_config = ASTConfig.from_pretrained(backbone_name)\n",
    "        self.encoder_model = ASTModel.from_pretrained(backbone_name, config=backbone_config)\n",
    "        self.hidden_size = self.encoder_model.config.hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch_size, t_len, f_len) input spectrograms\n",
    "        # output: (batch_size, seq_len, enc_dim) embeddings\n",
    "        \n",
    "        spec_embeddings = self.encoder_model.embeddings(x)\n",
    "        encoder_outputs = self.encoder_model.encoder(spec_embeddings)\n",
    "        \n",
    "        return encoder_outputs.last_hidden_state, spec_embeddings\n",
    "    \n",
    "    \n",
    "class BirdASTForPretrain(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone_name, mask_ratio=0.75, d_dim=768, n_layers=2, n_heads=6, ff_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = BirdEncoder(backbone_name)\n",
    "        \n",
    "        enc_dim = self.encoder.hidden_size\n",
    "        self.decoder = BirdDecoder(enc_dim, d_dim, n_layers, n_heads, ff_dim, dropout)\n",
    "        \n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, t_len, f_len) input spectrograms\n",
    "        # output: (batch_size, t_len, f_len) reconstructed spectrograms\n",
    "        \n",
    "        batch_size, t_len, f_len = x.size()\n",
    "        memory, spec_embeddings = self.encoder(x)\n",
    "        \n",
    "        # Mask indices\n",
    "        seq_len = memory.size(1)\n",
    "        mask_indices = torch.randint(0, seq_len, (batch_size, int(self.mask_ratio * seq_len)), device=x.device)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_tokens = self.decoder(spec_embeddings, memory, mask_indices)\n",
    "        \n",
    "        return decoded_tokens \n",
    "    \n",
    "\n",
    "def get_shape(config):\n",
    "    # see Karpathy's cs231n blog on how to calculate the output dimensions\n",
    "    # https://cs231n.github.io/convolutional-networks/#conv\n",
    "    frequency_out_dimension = (config.num_mel_bins - config.patch_size) // config.frequency_stride + 1\n",
    "    time_out_dimension = (config.max_length - config.patch_size) // config.time_stride + 1\n",
    "\n",
    "    return frequency_out_dimension, time_out_dimension\n",
    "\n",
    "\n",
    "def patchify_spectrogram(spectrograms, config):\n",
    "    # spectrograms: (batch_size, max_length, num_mel_bins)\n",
    "    \n",
    "    batch_size = spectrograms.size(0)\n",
    "    patch_size = config.patch_size\n",
    "    time_stride = config.time_stride\n",
    "    frequency_stride = config.frequency_stride\n",
    "\n",
    "    patches = spectrograms.unfold(1, patch_size, time_stride).unfold(2, patch_size, frequency_stride)\n",
    "    patches = patches.unsqueeze(3).expand(-1, -1, -1, 3, -1, -1)\n",
    "\n",
    "    num_patches_y = patches.size(1)\n",
    "    num_patches_x = patches.size(2)\n",
    "\n",
    "    flattened_patches = patches.contiguous().view(batch_size, num_patches_y * num_patches_x, -1)\n",
    "\n",
    "    # -> (batch_size, num_patches_y * num_patches_x, 3*patch_size*patch_size)\n",
    "    return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSongDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_audio_meta, config):\n",
    "        self.df_audio_meta = df_audio_meta\n",
    "        self.feature_extractor = ASTFeatureExtractor()\n",
    "        self.config = config\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_audio_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_audio_meta.iloc[idx]\n",
    "        audio_path = f\"{self.config.dataset_dir}/{row['file_name']}\"\n",
    "        audio_arr, sr = self.get_audio(audio_path)\n",
    "        spec = self.feature_extractor(audio_arr, sampling_rate=sr, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return spec['input_values'].squeeze(0), row['species_id']\n",
    "\n",
    "    def get_audio(self, audio_path):\n",
    "        audio, sr = librosa.load(audio_path, sr=self.config.audio_sr)\n",
    "        return audio, sr\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [x[0] for x in batch]\n",
    "    targets = [x[1] for x in batch]\n",
    "    data_dict = {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0),\n",
    "        \"labels\": torch.tensor(targets)\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the dataset, dataloader and patchify function\n",
    "# backbone_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "# backbone_config = ASTConfig.from_pretrained(backbone_name)\n",
    "\n",
    "# bs_dataset = BirdSongDataset(df_audio_meta, config)\n",
    "# bs_dataloader = DataLoader(bs_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# for batch in bs_dataloader:\n",
    "#     spectrograms = batch['input_ids']\n",
    "#     labels = batch['labels']\n",
    "#     break\n",
    "\n",
    "# spectrogram_patches = patchify_spectrogram(spectrograms, backbone_config)\n",
    "\n",
    "# print(spectrograms.size())\n",
    "# print(spectrogram_patches.size())\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(6, 12))\n",
    "# ax.imshow(spectrograms[0].cpu().numpy(), aspect='auto', vmax=1, vmin=-1)\n",
    "# ax.set_title(\"Spectrogram\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot patches\n",
    "# fig, axes = plt.subplots(time_dim, freq_dim, figsize=(freq_dim * 2, time_dim * 2))\n",
    "# patch_size = 16\n",
    "# for i in range(time_dim):\n",
    "#     for j in range(freq_dim):\n",
    "#         patch_index = i * freq_dim + j\n",
    "#         patch = spectrogram_patches[0, patch_index].view(3, patch_size, patch_size).permute(1, 2, 0).numpy()\n",
    "#         ax = axes[i, j]\n",
    "#         ax.imshow(patch[:, :, 0], aspect='auto',  vmax=1, vmin=-1)\n",
    "#         ax.set_title(f'Patch {patch_index + 1}')\n",
    "#         ax.axis('off') \n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.value = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.value = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "# train the BirdASTForPretrain with self-supervised learning\n",
    "\n",
    "backbone_config = ASTConfig.from_pretrained(config.backbone_name)\n",
    "\n",
    "bs_dataset = BirdSongDataset(df_audio_meta, config)\n",
    "bs_dataloader = DataLoader(bs_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = BirdASTForPretrain(\n",
    "    config.backbone_name, \n",
    "    mask_ratio=0.75, \n",
    "    d_dim=768, \n",
    "    n_layers=config.n_decoder_layers, \n",
    "    n_heads=config.n_decoder_heads, \n",
    "    ff_dim=config.ff_dim_decoder, \n",
    "    dropout=0.0\n",
    "    )\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config.max_lr, weight_decay=config.weight_decay)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=config.max_lr, \n",
    "    final_div_factor=config.lr_final_div, \n",
    "    steps_per_epoch=len(bs_dataloader), \n",
    "    epochs=config.epochs\n",
    "    )\n",
    "\n",
    "scaler = GradScaler(enabled=config.amp)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "loss_records = defaultdict(list)\n",
    "\n",
    "wandb_init(\"BirdAST_Pretrain\", \"BirdAST_Pretrain_Large\", config)\n",
    "logger = get_logger(f\"{config.log_dir}/BirdAST_Pretrain.log\")\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(bs_dataloader), total=len(bs_dataloader)):\n",
    "        \n",
    "        spectrograms = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=config.amp):\n",
    "            reconstructed_spectrograms = model(spectrograms)\n",
    "            spectrogram_patches = patchify_spectrogram(spectrograms, backbone_config)\n",
    "            loss = loss_fn(reconstructed_spectrograms[:, 2:, :], spectrogram_patches)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        loss_meter.update(loss.item())\n",
    "        \n",
    "        wandb.log({\"Loss\": loss.item(), \"Learning Rate\": scheduler.get_last_lr()[0], \"Epoch\": epoch, \"Batch\": i})\n",
    "        \n",
    "        if i % config.print_freq == 0:\n",
    "            logger.info(f\"Epoch: {epoch} | Batch: {i}, Loss: {loss_meter.avg}\")\n",
    "            \n",
    "    wandb.log({\"Epoch_Loss\": loss_meter.avg, \"Epoch\": epoch})\n",
    "    loss_records[f'epoch_{i+1}'].append(loss_meter.avg)\n",
    "    logger.info('-'*10 + \"\\n\" + f\"Epoch: {epoch} | Loss: {loss_meter.avg}\" + \"\\n\" + '-'*10)\n",
    "    \n",
    "    # save model\n",
    "    model_path = f\"{config.log_dir}/BirdAST_Pretrain_epoch_{epoch}.pth\"\n",
    "    if loss_meter.avg < best_loss:\n",
    "        best_loss = loss_meter.avg\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        logger.info(f\"Best Model loss = {loss_meter.avg} | saved at: {model_path}\")\n",
    "        \n",
    "    # clear memory\n",
    "    del spectrograms, labels, reconstructed_spectrograms, spectrogram_patches \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in tqdm(enumerate(bs_dataloader), total=len(bs_dataloader)):\n",
    "        \n",
    "    spectrograms = batch['input_ids'].to(DEVICE)\n",
    "    labels = batch['labels'].to(DEVICE)\n",
    "    \n",
    "    reconstructed_spectrograms = model(spectrograms)\n",
    "        \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_patches = patchify_spectrogram(spectrograms, backbone_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpathify(spec_patches, time_dim=101, freq_dim=12, patch_size=16, n_channels=3):\n",
    "    # spec_patches: (batch_size, num_patches, patch_size*patch_size*3\n",
    "    \n",
    "    batch_size, num_patches, d_dim = spec_patches.size()\n",
    "    \n",
    "    patches = spec_patches.view(batch_size, time_dim, freq_dim, n_channels, patch_size, patch_size)\n",
    "   \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_unpatched = unpathify(spectrogram_patches)\n",
    "\n",
    "spec_unpatched.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot patches\n",
    "\n",
    "time_dim = 101\n",
    "freq_dim = 12\n",
    "\n",
    "fig, axes = plt.subplots(time_dim, freq_dim, figsize=(freq_dim * 2, time_dim * 2))\n",
    "patch_size = 16\n",
    "for i in range(time_dim):\n",
    "    for j in range(freq_dim):\n",
    "        patch = spec_unpatched[0, i, j].cpu().numpy()\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(patch[0, :, :], aspect='auto',  vmax=1, vmin=-1)\n",
    "        ax.set_title(f'Patch {i}, {j}')\n",
    "        ax.axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_specs_unpatched = unpathify(reconstructed_spectrograms[:, 2:, :])\n",
    "\n",
    "print(reconstructed_specs_unpatched.size())\n",
    "\n",
    "fig, axes = plt.subplots(time_dim, freq_dim, figsize=(freq_dim * 2, time_dim * 2))\n",
    "patch_size = 16\n",
    "for i in range(time_dim):\n",
    "    for j in range(freq_dim):\n",
    "        patch = reconstructed_specs_unpatched[0, i, j].detach().cpu().numpy()\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(patch[0, :, :], aspect='auto',  vmax=1, vmin=-1)\n",
    "        ax.set_title(f'Patch {i}, {j}')\n",
    "        ax.axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Convert ViT and non-distilled DeiT checkpoints from the timm library.\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from timm.data import ImageNetInfo, infer_imagenet_subset\n",
    "\n",
    "from transformers import DeiTImageProcessor, ViTConfig, ViTForImageClassification, ViTImageProcessor, ViTModel\n",
    "from transformers.utils import logging\n",
    "\n",
    "from transformers import ViTMAEConfig, ViTMAEForPreTraining, ViTMAEModel\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "# here we list all keys to be renamed (original name on the left, our name on the right)\n",
    "def create_rename_keys(config, base_model=False):\n",
    "    rename_keys = []\n",
    "    for i in range(config.num_hidden_layers):\n",
    "        # encoder layers: output projection, 2 feedforward neural networks and 2 layernorms\n",
    "        rename_keys.append((f\"blocks.{i}.norm1.weight\", f\"vit.encoder.layer.{i}.layernorm_before.weight\"))\n",
    "        rename_keys.append((f\"blocks.{i}.norm1.bias\", f\"vit.encoder.layer.{i}.layernorm_before.bias\"))\n",
    "        rename_keys.append((f\"blocks.{i}.attn.proj.weight\", f\"vit.encoder.layer.{i}.attention.output.dense.weight\"))\n",
    "        rename_keys.append((f\"blocks.{i}.attn.proj.bias\", f\"vit.encoder.layer.{i}.attention.output.dense.bias\"))\n",
    "        rename_keys.append((f\"blocks.{i}.norm2.weight\", f\"vit.encoder.layer.{i}.layernorm_after.weight\"))\n",
    "        rename_keys.append((f\"blocks.{i}.norm2.bias\", f\"vit.encoder.layer.{i}.layernorm_after.bias\"))\n",
    "        rename_keys.append((f\"blocks.{i}.mlp.fc1.weight\", f\"vit.encoder.layer.{i}.intermediate.dense.weight\"))\n",
    "        rename_keys.append((f\"blocks.{i}.mlp.fc1.bias\", f\"vit.encoder.layer.{i}.intermediate.dense.bias\"))\n",
    "        rename_keys.append((f\"blocks.{i}.mlp.fc2.weight\", f\"vit.encoder.layer.{i}.output.dense.weight\"))\n",
    "        rename_keys.append((f\"blocks.{i}.mlp.fc2.bias\", f\"vit.encoder.layer.{i}.output.dense.bias\"))\n",
    "\n",
    "    # projection layer + position embeddings\n",
    "    rename_keys.extend(\n",
    "        [\n",
    "            (\"cls_token\", \"vit.embeddings.cls_token\"),\n",
    "            (\"patch_embed.proj.weight\", \"vit.embeddings.patch_embeddings.projection.weight\"),\n",
    "            (\"patch_embed.proj.bias\", \"vit.embeddings.patch_embeddings.projection.bias\"),\n",
    "            (\"pos_embed\", \"vit.embeddings.position_embeddings\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if base_model:\n",
    "        # layernorm\n",
    "        rename_keys.extend(\n",
    "            [\n",
    "                (\"norm.weight\", \"layernorm.weight\"),\n",
    "                (\"norm.bias\", \"layernorm.bias\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # if just the base model, we should remove \"vit\" from all keys that start with \"vit\"\n",
    "        rename_keys = [(pair[0], pair[1][4:]) if pair[1].startswith(\"vit\") else pair for pair in rename_keys]\n",
    "    else:\n",
    "        # layernorm + classification head\n",
    "        rename_keys.extend(\n",
    "            [\n",
    "                (\"norm.weight\", \"vit.layernorm.weight\"),\n",
    "                (\"norm.bias\", \"vit.layernorm.bias\"),\n",
    "                (\"head.weight\", \"classifier.weight\"),\n",
    "                (\"head.bias\", \"classifier.bias\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return rename_keys\n",
    "\n",
    "\n",
    "# we split up the matrix of each encoder layer into queries, keys and values\n",
    "def read_in_q_k_v(state_dict, config, base_model=False):\n",
    "    for i in range(config.num_hidden_layers):\n",
    "        if base_model:\n",
    "            prefix = \"\"\n",
    "        else:\n",
    "            prefix = \"vit.\"\n",
    "        # read in weights + bias of input projection layer (in timm, this is a single matrix + bias)\n",
    "        in_proj_weight = state_dict.pop(f\"blocks.{i}.attn.qkv.weight\")\n",
    "        in_proj_bias = state_dict.pop(f\"blocks.{i}.attn.qkv.bias\")\n",
    "        # next, add query, keys and values (in that order) to the state dict\n",
    "        state_dict[f\"{prefix}encoder.layer.{i}.attention.attention.query.weight\"] = in_proj_weight[\n",
    "            : config.hidden_size, :\n",
    "        ]\n",
    "        state_dict[f\"{prefix}encoder.layer.{i}.attention.attention.query.bias\"] = in_proj_bias[: config.hidden_size]\n",
    "        state_dict[f\"{prefix}encoder.layer.{i}.attention.attention.key.weight\"] = in_proj_weight[\n",
    "            config.hidden_size : config.hidden_size * 2, :\n",
    "        ]\n",
    "        state_dict[f\"{prefix}encoder.layer.{i}.attention.attention.key.bias\"] = in_proj_bias[\n",
    "            config.hidden_size : config.hidden_size * 2\n",
    "        ]\n",
    "        state_dict[f\"{prefix}encoder.layer.{i}.attention.attention.value.weight\"] = in_proj_weight[\n",
    "            -config.hidden_size :, :\n",
    "        ]\n",
    "        state_dict[f\"{prefix}encoder.layer.{i}.attention.attention.value.bias\"] = in_proj_bias[-config.hidden_size :]\n",
    "\n",
    "\n",
    "def remove_classification_head_(state_dict):\n",
    "    ignore_keys = [\"head.weight\", \"head.bias\"]\n",
    "    for k in ignore_keys:\n",
    "        state_dict.pop(k, None)\n",
    "\n",
    "\n",
    "def rename_key(dct, old, new):\n",
    "    val = dct.pop(old)\n",
    "    dct[new] = val\n",
    "\n",
    "\n",
    "# We will verify our results on an image of cute cats\n",
    "def prepare_img():\n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    im = Image.open(requests.get(url, stream=True).raw)\n",
    "    return im\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def convert_vit_checkpoint(vit_name, pytorch_dump_folder_path):\n",
    "    \"\"\"\n",
    "    Copy/paste/tweak model's weights to our ViT structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # define default ViT configuration\n",
    "    config = ViTMAEConfig()\n",
    "    base_model = False\n",
    "\n",
    "    # load original model from timm\n",
    "    timm_model = timm.create_model(vit_name, pretrained=True, num_classes=0)\n",
    "    timm_model.eval()\n",
    "    \n",
    "    print(timm_model.num_classes)\n",
    "    \n",
    "    print(getattr(timm_model, \"fc_norm\", None), getattr(timm_model, \"global_pool\", None))\n",
    "\n",
    "    # detect unsupported ViT models in transformers\n",
    "    # fc_norm is present\n",
    "    if not isinstance(getattr(timm_model, \"fc_norm\", None), torch.nn.Identity):\n",
    "        raise ValueError(f\"{vit_name} is not supported in transformers because of the presence of fc_norm.\")\n",
    "\n",
    "    # use of global average pooling in combination (or without) class token\n",
    "    if getattr(timm_model, \"global_pool\", None) == \"avg\":\n",
    "        raise ValueError(f\"{vit_name} is not supported in transformers because of use of global average pooling.\")\n",
    "\n",
    "    # CLIP style vit with norm_pre layer present\n",
    "    if \"clip\" in vit_name and not isinstance(getattr(timm_model, \"norm_pre\", None), torch.nn.Identity):\n",
    "        raise ValueError(\n",
    "            f\"{vit_name} is not supported in transformers because it's a CLIP style ViT with norm_pre layer.\"\n",
    "        )\n",
    "\n",
    "    # SigLIP style vit with attn_pool layer present\n",
    "    if \"siglip\" in vit_name and getattr(timm_model, \"global_pool\", None) == \"map\":\n",
    "        raise ValueError(\n",
    "            f\"{vit_name} is not supported in transformers because it's a SigLIP style ViT with attn_pool.\"\n",
    "        )\n",
    "\n",
    "    # use of layer scale in ViT model blocks\n",
    "    if not isinstance(getattr(timm_model.blocks[0], \"ls1\", None), torch.nn.Identity) or not isinstance(\n",
    "        getattr(timm_model.blocks[0], \"ls2\", None), torch.nn.Identity\n",
    "    ):\n",
    "        raise ValueError(f\"{vit_name} is not supported in transformers because it uses a layer scale in its blocks.\")\n",
    "\n",
    "    # Hybrid ResNet-ViTs\n",
    "    if not isinstance(timm_model.patch_embed, timm.layers.PatchEmbed):\n",
    "        raise ValueError(f\"{vit_name} is not supported in transformers because it is a hybrid ResNet-ViT.\")\n",
    "\n",
    "    # get patch size and image size from the patch embedding submodule\n",
    "    config.patch_size = (\n",
    "        timm_model.patch_embed.patch_size[0],\n",
    "        timm_model.patch_embed.patch_size[1]\n",
    "        )\n",
    "    config.image_size = (\n",
    "        timm_model.patch_embed.img_size[0],\n",
    "        timm_model.patch_embed.img_size[1]\n",
    "        )\n",
    "\n",
    "    # retrieve architecture-specific parameters from the timm model\n",
    "    config.hidden_size = timm_model.embed_dim\n",
    "    config.intermediate_size = timm_model.blocks[0].mlp.fc1.out_features\n",
    "    config.num_hidden_layers = len(timm_model.blocks)\n",
    "    config.num_attention_heads = timm_model.blocks[0].attn.num_heads\n",
    "    config.num_channels = 1\n",
    "\n",
    "    # check whether the model has a classification head or not\n",
    "    if timm_model.num_classes != 0:\n",
    "        config.num_labels = timm_model.num_classes\n",
    "        # infer ImageNet subset from timm model\n",
    "        imagenet_subset = infer_imagenet_subset(timm_model)\n",
    "        dataset_info = ImageNetInfo(imagenet_subset)\n",
    "        config.id2label = {i: dataset_info.index_to_label_name(i) for i in range(dataset_info.num_classes())}\n",
    "        config.label2id = {v: k for k, v in config.id2label.items()}\n",
    "    else:\n",
    "        print(f\"{vit_name} is going to be converted as a feature extractor only.\")\n",
    "        base_model = True\n",
    "\n",
    "    # load state_dict of original model\n",
    "    state_dict = timm_model.state_dict()\n",
    "\n",
    "    # remove and rename some keys in the state dict\n",
    "    if base_model:\n",
    "        remove_classification_head_(state_dict)\n",
    "    rename_keys = create_rename_keys(config, base_model)\n",
    "    for src, dest in rename_keys:\n",
    "        rename_key(state_dict, src, dest)\n",
    "    read_in_q_k_v(state_dict, config, base_model)\n",
    "\n",
    "    print(config)\n",
    "    \n",
    "    # load HuggingFace model\n",
    "    if base_model:\n",
    "        model = ViTMAEModel(config).eval()\n",
    "    else:\n",
    "        model = ViTForImageClassification(config).eval()\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Check outputs on an image, prepared by ViTImageProcessor/DeiTImageProcessor\n",
    "    if \"deit\" in vit_name:\n",
    "        image_processor = DeiTImageProcessor(size=config.image_size)\n",
    "    else:\n",
    "        image_processor = ViTImageProcessor(size=config.image_size)\n",
    "    encoding = image_processor(images=prepare_img(), return_tensors=\"pt\")\n",
    "    pixel_values = encoding[\"pixel_values\"]\n",
    "    outputs = model(pixel_values)\n",
    "\n",
    "    if base_model:\n",
    "        timm_pooled_output = timm_model.forward_features(pixel_values)\n",
    "        assert timm_pooled_output.shape == outputs.last_hidden_state.shape\n",
    "        assert torch.allclose(timm_pooled_output, outputs.last_hidden_state, atol=1e-1)\n",
    "    else:\n",
    "        timm_logits = timm_model(pixel_values)\n",
    "        assert timm_logits.shape == outputs.logits.shape\n",
    "        assert torch.allclose(timm_logits, outputs.logits, atol=1e-3)\n",
    "\n",
    "    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n",
    "    print(f\"Saving model {vit_name} to {pytorch_dump_folder_path}\")\n",
    "    model.save_pretrained(pytorch_dump_folder_path)\n",
    "    print(f\"Saving image processor to {pytorch_dump_folder_path}\")\n",
    "    image_processor.save_pretrained(pytorch_dump_folder_path)\n",
    "\n",
    "vit_name = \"hf_hub:gaunernst/vit_base_patch16_1024_128.audiomae_as2m\"\n",
    "convert_vit_checkpoint(vit_name, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024//16 * 128//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [1, 768]                  394,752\n",
       "├─PatchEmbed: 1-1                        [1, 512, 768]             --\n",
       "│    └─Conv2d: 2-1                       [1, 768, 64, 8]           197,376\n",
       "│    └─Identity: 2-2                     [1, 512, 768]             --\n",
       "├─Dropout: 1-2                           [1, 513, 768]             --\n",
       "├─Identity: 1-3                          [1, 513, 768]             --\n",
       "├─Identity: 1-4                          [1, 513, 768]             --\n",
       "├─Sequential: 1-5                        [1, 513, 768]             --\n",
       "│    └─Block: 2-3                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-1               [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-2               [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-3                [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-4                [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-5               [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-6                     [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-7                [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-8                [1, 513, 768]             --\n",
       "│    └─Block: 2-4                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-10              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-11               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-12               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-13              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-14                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-15               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-16               [1, 513, 768]             --\n",
       "│    └─Block: 2-5                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-17              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-18              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-19               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-20               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-22                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-23               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-24               [1, 513, 768]             --\n",
       "│    └─Block: 2-6                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-25              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-26              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-27               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-28               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-29              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-30                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-31               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-32               [1, 513, 768]             --\n",
       "│    └─Block: 2-7                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-34              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-35               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-36               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-37              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-38                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-39               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-40               [1, 513, 768]             --\n",
       "│    └─Block: 2-8                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-41              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-42              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-43               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-44               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-45              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-46                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-47               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-48               [1, 513, 768]             --\n",
       "│    └─Block: 2-9                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-49              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-50              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-51               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-52               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-53              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-54                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-55               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-56               [1, 513, 768]             --\n",
       "│    └─Block: 2-10                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-57              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-58              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-59               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-60               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-61              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-62                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-63               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-64               [1, 513, 768]             --\n",
       "│    └─Block: 2-11                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-65              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-66              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-67               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-68               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-69              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-70                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-71               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-72               [1, 513, 768]             --\n",
       "│    └─Block: 2-12                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-73              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-74              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-75               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-76               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-77              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-78                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-79               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-80               [1, 513, 768]             --\n",
       "│    └─Block: 2-13                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-81              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-82              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-83               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-84               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-85              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-86                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-87               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-88               [1, 513, 768]             --\n",
       "│    └─Block: 2-14                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-89              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-90              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-91               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-92               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-93              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-94                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-95               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-96               [1, 513, 768]             --\n",
       "├─LayerNorm: 1-6                         [1, 513, 768]             1,536\n",
       "├─Identity: 1-7                          [1, 768]                  --\n",
       "├─Dropout: 1-8                           [1, 768]                  --\n",
       "├─Identity: 1-9                          [1, 768]                  --\n",
       "==========================================================================================\n",
       "Total params: 85,648,128\n",
       "Trainable params: 85,648,128\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 186.11\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 422.34\n",
       "Params size (MB): 341.01\n",
       "Estimated Total Size (MB): 763.88\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.compliance import kaldi\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "# NOTE: for timm<0.9.11, you also need to pass `global_pool='avg'`\n",
    "# if only embeddings are needed, pass `num_classes=0`\n",
    "model = timm.create_model(\"hf_hub:gaunernst/vit_base_patch16_1024_128.audiomae_as2m\", pretrained=True, num_classes=0)\n",
    "\n",
    "# sub_model = nn.Sequential(*list(model.children())[0])\n",
    "patch_embed = list(model.children())[0]\n",
    "\n",
    "melspec = torch.randn(1, 1, 1024, 128)\n",
    "\n",
    "output = patch_embed(melspec)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "summary(model, input_size=(1, 1, 1024, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls_token torch.Size([1, 1, 768])\n",
      "1 pos_embed torch.Size([1, 513, 768])\n",
      "2 patch_embed.proj.weight torch.Size([768, 1, 16, 16])\n",
      "3 patch_embed.proj.bias torch.Size([768])\n",
      "4 blocks.0.norm1.weight torch.Size([768])\n",
      "5 blocks.0.norm1.bias torch.Size([768])\n",
      "6 blocks.0.attn.qkv.weight torch.Size([2304, 768])\n",
      "7 blocks.0.attn.qkv.bias torch.Size([2304])\n",
      "8 blocks.0.attn.proj.weight torch.Size([768, 768])\n",
      "9 blocks.0.attn.proj.bias torch.Size([768])\n",
      "10 blocks.0.norm2.weight torch.Size([768])\n",
      "11 blocks.0.norm2.bias torch.Size([768])\n",
      "12 blocks.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "13 blocks.0.mlp.fc1.bias torch.Size([3072])\n",
      "14 blocks.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "15 blocks.0.mlp.fc2.bias torch.Size([768])\n",
      "16 blocks.1.norm1.weight torch.Size([768])\n",
      "17 blocks.1.norm1.bias torch.Size([768])\n",
      "18 blocks.1.attn.qkv.weight torch.Size([2304, 768])\n",
      "19 blocks.1.attn.qkv.bias torch.Size([2304])\n",
      "20 blocks.1.attn.proj.weight torch.Size([768, 768])\n",
      "21 blocks.1.attn.proj.bias torch.Size([768])\n",
      "22 blocks.1.norm2.weight torch.Size([768])\n",
      "23 blocks.1.norm2.bias torch.Size([768])\n",
      "24 blocks.1.mlp.fc1.weight torch.Size([3072, 768])\n",
      "25 blocks.1.mlp.fc1.bias torch.Size([3072])\n",
      "26 blocks.1.mlp.fc2.weight torch.Size([768, 3072])\n",
      "27 blocks.1.mlp.fc2.bias torch.Size([768])\n",
      "28 blocks.2.norm1.weight torch.Size([768])\n",
      "29 blocks.2.norm1.bias torch.Size([768])\n",
      "30 blocks.2.attn.qkv.weight torch.Size([2304, 768])\n",
      "31 blocks.2.attn.qkv.bias torch.Size([2304])\n",
      "32 blocks.2.attn.proj.weight torch.Size([768, 768])\n",
      "33 blocks.2.attn.proj.bias torch.Size([768])\n",
      "34 blocks.2.norm2.weight torch.Size([768])\n",
      "35 blocks.2.norm2.bias torch.Size([768])\n",
      "36 blocks.2.mlp.fc1.weight torch.Size([3072, 768])\n",
      "37 blocks.2.mlp.fc1.bias torch.Size([3072])\n",
      "38 blocks.2.mlp.fc2.weight torch.Size([768, 3072])\n",
      "39 blocks.2.mlp.fc2.bias torch.Size([768])\n",
      "40 blocks.3.norm1.weight torch.Size([768])\n",
      "41 blocks.3.norm1.bias torch.Size([768])\n",
      "42 blocks.3.attn.qkv.weight torch.Size([2304, 768])\n",
      "43 blocks.3.attn.qkv.bias torch.Size([2304])\n",
      "44 blocks.3.attn.proj.weight torch.Size([768, 768])\n",
      "45 blocks.3.attn.proj.bias torch.Size([768])\n",
      "46 blocks.3.norm2.weight torch.Size([768])\n",
      "47 blocks.3.norm2.bias torch.Size([768])\n",
      "48 blocks.3.mlp.fc1.weight torch.Size([3072, 768])\n",
      "49 blocks.3.mlp.fc1.bias torch.Size([3072])\n",
      "50 blocks.3.mlp.fc2.weight torch.Size([768, 3072])\n",
      "51 blocks.3.mlp.fc2.bias torch.Size([768])\n",
      "52 blocks.4.norm1.weight torch.Size([768])\n",
      "53 blocks.4.norm1.bias torch.Size([768])\n",
      "54 blocks.4.attn.qkv.weight torch.Size([2304, 768])\n",
      "55 blocks.4.attn.qkv.bias torch.Size([2304])\n",
      "56 blocks.4.attn.proj.weight torch.Size([768, 768])\n",
      "57 blocks.4.attn.proj.bias torch.Size([768])\n",
      "58 blocks.4.norm2.weight torch.Size([768])\n",
      "59 blocks.4.norm2.bias torch.Size([768])\n",
      "60 blocks.4.mlp.fc1.weight torch.Size([3072, 768])\n",
      "61 blocks.4.mlp.fc1.bias torch.Size([3072])\n",
      "62 blocks.4.mlp.fc2.weight torch.Size([768, 3072])\n",
      "63 blocks.4.mlp.fc2.bias torch.Size([768])\n",
      "64 blocks.5.norm1.weight torch.Size([768])\n",
      "65 blocks.5.norm1.bias torch.Size([768])\n",
      "66 blocks.5.attn.qkv.weight torch.Size([2304, 768])\n",
      "67 blocks.5.attn.qkv.bias torch.Size([2304])\n",
      "68 blocks.5.attn.proj.weight torch.Size([768, 768])\n",
      "69 blocks.5.attn.proj.bias torch.Size([768])\n",
      "70 blocks.5.norm2.weight torch.Size([768])\n",
      "71 blocks.5.norm2.bias torch.Size([768])\n",
      "72 blocks.5.mlp.fc1.weight torch.Size([3072, 768])\n",
      "73 blocks.5.mlp.fc1.bias torch.Size([3072])\n",
      "74 blocks.5.mlp.fc2.weight torch.Size([768, 3072])\n",
      "75 blocks.5.mlp.fc2.bias torch.Size([768])\n",
      "76 blocks.6.norm1.weight torch.Size([768])\n",
      "77 blocks.6.norm1.bias torch.Size([768])\n",
      "78 blocks.6.attn.qkv.weight torch.Size([2304, 768])\n",
      "79 blocks.6.attn.qkv.bias torch.Size([2304])\n",
      "80 blocks.6.attn.proj.weight torch.Size([768, 768])\n",
      "81 blocks.6.attn.proj.bias torch.Size([768])\n",
      "82 blocks.6.norm2.weight torch.Size([768])\n",
      "83 blocks.6.norm2.bias torch.Size([768])\n",
      "84 blocks.6.mlp.fc1.weight torch.Size([3072, 768])\n",
      "85 blocks.6.mlp.fc1.bias torch.Size([3072])\n",
      "86 blocks.6.mlp.fc2.weight torch.Size([768, 3072])\n",
      "87 blocks.6.mlp.fc2.bias torch.Size([768])\n",
      "88 blocks.7.norm1.weight torch.Size([768])\n",
      "89 blocks.7.norm1.bias torch.Size([768])\n",
      "90 blocks.7.attn.qkv.weight torch.Size([2304, 768])\n",
      "91 blocks.7.attn.qkv.bias torch.Size([2304])\n",
      "92 blocks.7.attn.proj.weight torch.Size([768, 768])\n",
      "93 blocks.7.attn.proj.bias torch.Size([768])\n",
      "94 blocks.7.norm2.weight torch.Size([768])\n",
      "95 blocks.7.norm2.bias torch.Size([768])\n",
      "96 blocks.7.mlp.fc1.weight torch.Size([3072, 768])\n",
      "97 blocks.7.mlp.fc1.bias torch.Size([3072])\n",
      "98 blocks.7.mlp.fc2.weight torch.Size([768, 3072])\n",
      "99 blocks.7.mlp.fc2.bias torch.Size([768])\n",
      "100 blocks.8.norm1.weight torch.Size([768])\n",
      "101 blocks.8.norm1.bias torch.Size([768])\n",
      "102 blocks.8.attn.qkv.weight torch.Size([2304, 768])\n",
      "103 blocks.8.attn.qkv.bias torch.Size([2304])\n",
      "104 blocks.8.attn.proj.weight torch.Size([768, 768])\n",
      "105 blocks.8.attn.proj.bias torch.Size([768])\n",
      "106 blocks.8.norm2.weight torch.Size([768])\n",
      "107 blocks.8.norm2.bias torch.Size([768])\n",
      "108 blocks.8.mlp.fc1.weight torch.Size([3072, 768])\n",
      "109 blocks.8.mlp.fc1.bias torch.Size([3072])\n",
      "110 blocks.8.mlp.fc2.weight torch.Size([768, 3072])\n",
      "111 blocks.8.mlp.fc2.bias torch.Size([768])\n",
      "112 blocks.9.norm1.weight torch.Size([768])\n",
      "113 blocks.9.norm1.bias torch.Size([768])\n",
      "114 blocks.9.attn.qkv.weight torch.Size([2304, 768])\n",
      "115 blocks.9.attn.qkv.bias torch.Size([2304])\n",
      "116 blocks.9.attn.proj.weight torch.Size([768, 768])\n",
      "117 blocks.9.attn.proj.bias torch.Size([768])\n",
      "118 blocks.9.norm2.weight torch.Size([768])\n",
      "119 blocks.9.norm2.bias torch.Size([768])\n",
      "120 blocks.9.mlp.fc1.weight torch.Size([3072, 768])\n",
      "121 blocks.9.mlp.fc1.bias torch.Size([3072])\n",
      "122 blocks.9.mlp.fc2.weight torch.Size([768, 3072])\n",
      "123 blocks.9.mlp.fc2.bias torch.Size([768])\n",
      "124 blocks.10.norm1.weight torch.Size([768])\n",
      "125 blocks.10.norm1.bias torch.Size([768])\n",
      "126 blocks.10.attn.qkv.weight torch.Size([2304, 768])\n",
      "127 blocks.10.attn.qkv.bias torch.Size([2304])\n",
      "128 blocks.10.attn.proj.weight torch.Size([768, 768])\n",
      "129 blocks.10.attn.proj.bias torch.Size([768])\n",
      "130 blocks.10.norm2.weight torch.Size([768])\n",
      "131 blocks.10.norm2.bias torch.Size([768])\n",
      "132 blocks.10.mlp.fc1.weight torch.Size([3072, 768])\n",
      "133 blocks.10.mlp.fc1.bias torch.Size([3072])\n",
      "134 blocks.10.mlp.fc2.weight torch.Size([768, 3072])\n",
      "135 blocks.10.mlp.fc2.bias torch.Size([768])\n",
      "136 blocks.11.norm1.weight torch.Size([768])\n",
      "137 blocks.11.norm1.bias torch.Size([768])\n",
      "138 blocks.11.attn.qkv.weight torch.Size([2304, 768])\n",
      "139 blocks.11.attn.qkv.bias torch.Size([2304])\n",
      "140 blocks.11.attn.proj.weight torch.Size([768, 768])\n",
      "141 blocks.11.attn.proj.bias torch.Size([768])\n",
      "142 blocks.11.norm2.weight torch.Size([768])\n",
      "143 blocks.11.norm2.bias torch.Size([768])\n",
      "144 blocks.11.mlp.fc1.weight torch.Size([3072, 768])\n",
      "145 blocks.11.mlp.fc1.bias torch.Size([3072])\n",
      "146 blocks.11.mlp.fc2.weight torch.Size([768, 3072])\n",
      "147 blocks.11.mlp.fc2.bias torch.Size([768])\n",
      "148 norm.weight torch.Size([768])\n",
      "149 norm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTMAEConfig, ViTMAEForPreTraining, ViTMAEModel\n",
    "\n",
    "model_name = \"gaunernst/vit_base_patch16_1024_128.audiomae_as2m_ft_as20k\"\n",
    "\n",
    "config = ViTMAEConfig.from_pretrained(model_name)\n",
    "# model = ViTMAEForPreTraining.from_pretrained(model_name, config=config)\n",
    "model = ViTMAEModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timm.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
