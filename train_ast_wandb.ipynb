{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import ASTConfig, ASTFeatureExtractor, ASTModel\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from time import time\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print system info \n",
    "!nvidia-smi\n",
    "!python --version\n",
    "\n",
    "# print pytorch lightning version\n",
    "print(f\"Pytorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(log_file='log.txt'):\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    # Logging to file\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    # Logging to console\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def wandb_init(project_name, run_name, config):\n",
    "    config_dict = {\n",
    "        k: v for k, v in config.__dict__.items() if not k.startswith('_') and not callable(v) and k != 'copy'\n",
    "    }\n",
    "    run = wandb.init(project=project_name, name=run_name, config=config_dict)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_FOLDER = \".\" #\"/content/drive/MyDrive/Colab Notebooks\"\n",
    "KEEP_COLS = ['category_number', 'common_name', 'audio_length', 'type', 'remarks', 'quality', 'scientific_name', 'mp3_link', 'region']\n",
    "\n",
    "class Config:\n",
    "    dataset_dir = f\"{DRIVE_FOLDER}/Audio_XenoCanto\"\n",
    "    labels_list = f\"{DRIVE_FOLDER}/xeno_labels.csv\"\n",
    "    model_name = \"BirdAST_Baseline_GroupKFold\"\n",
    "    backbone_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "    n_classes = 728 # number of classes in the dataset\n",
    "    audio_sr = 16000 #Hz\n",
    "    segment_length = 10  #s\n",
    "    fft_window = 0.025 #s\n",
    "    hop_window_length = 0.01 #s\n",
    "    n_mels = 128\n",
    "    low_cut = 1000 #Hz\n",
    "    high_cut = 8000 #Hz\n",
    "    top_db = 100\n",
    "    batch_size = 16 #4 \n",
    "    num_workers = 0\n",
    "    n_splits = 5\n",
    "    log_dir = f\"{DRIVE_FOLDER}/training_logs\"\n",
    "    max_lr = 1e-5\n",
    "    epochs = 10\n",
    "    weight_decay = 0.01\n",
    "    lr_final_div = 1000\n",
    "    amp = True\n",
    "    grad_accum_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "    print_epoch_freq = 1\n",
    "    print_freq = 500\n",
    "    random_seed = 2046\n",
    "    \n",
    "    @classmethod\n",
    "    def copy(cls):\n",
    "        new_class = type('CustomConfig', (cls,), {k: v for k, v in cls.__dict__.items() if not k.startswith('__') and not callable(v)})\n",
    "        return new_class\n",
    "    \n",
    "config = Config.copy()\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed_everything(config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audio_meta = pd.read_csv(f\"{config.dataset_dir}/metadata.csv\", nrows=None)\n",
    "df_audio_meta = df_audio_meta.dropna().reset_index(drop=True)\n",
    "\n",
    "# Filter out files that do not exist\n",
    "df_audio_meta['file_exists'] = df_audio_meta['file_name'].apply(lambda x: os.path.exists(f\"{config.dataset_dir}/{x}\"))\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['file_exists']].reset_index(drop=True)\n",
    "\n",
    "# parse scientific names\n",
    "df_audio_meta['scientific_name'] = df_audio_meta['scientific_name'].apply(lambda x: \"_\".join(x.split(\" \")))\n",
    "\n",
    "# drop species with less than 2 samples\n",
    "class_counts = df_audio_meta['scientific_name'].value_counts()\n",
    "print(f\"Number of classes with less than 2 samples: {len(class_counts[class_counts < 2])}\")\n",
    "\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['scientific_name'].isin(class_counts[class_counts > 1].index)].copy().reset_index(drop=True)\n",
    "\n",
    "# encode scientific names to label ids\n",
    "label_ids_list = df_audio_meta['scientific_name'].unique().tolist()\n",
    "label_ids_list.sort()\n",
    "label_to_id = {label: i for i, label in enumerate(label_ids_list)}\n",
    "df_audio_meta['species_id'] = df_audio_meta['scientific_name'].map(label_to_id)\n",
    "\n",
    "# save the label mapping\n",
    "label_mapping = pd.DataFrame(label_to_id.items(), columns=['scientific_name', 'species_id'])\n",
    "label_mapping.to_csv(f\"{config.log_dir}/{config.model_name}_label_map.csv\", index=False)\n",
    "\n",
    "# drop samples with no labels\n",
    "df_audio_meta.dropna(subset=['species_id'], inplace=True)\n",
    "df_audio_meta.reset_index(drop=True, inplace=True)\n",
    "df_audio_meta['species_id'] = df_audio_meta['species_id'].astype(int)\n",
    "\n",
    "print(f\"Number of classes in dataset: {df_audio_meta['species_id'].nunique()}\")\n",
    "print(f'Number of samples:', len(df_audio_meta))\n",
    "\n",
    "# save the number of classes in the config\n",
    "config.n_classes = df_audio_meta['species_id'].nunique()\n",
    "\n",
    "# encode mp3 links to group ids for 5-folds\n",
    "group_ids = df_audio_meta['mp3_link'].unique().tolist()\n",
    "group_ids_map = {group_id: i for i, group_id in enumerate(group_ids)}\n",
    "df_audio_meta['group_id'] = df_audio_meta['mp3_link'].map(group_ids_map)\n",
    "\n",
    "df_audio_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all species in greater_manaus are in amazonas\n",
    "greater_manaus = df_audio_meta[df_audio_meta['region'] == 'greater_manaus']\n",
    "amazonas = df_audio_meta[df_audio_meta['region'] == 'amazonas']\n",
    "\n",
    "print('Number of species in greater_manaus:', len(greater_manaus['scientific_name'].unique()))\n",
    "print('Number of species in amazonas:', len(amazonas['scientific_name'].unique()))\n",
    "\n",
    "shared_species = [x for x in greater_manaus['scientific_name'].unique().tolist() if x in amazonas['scientific_name'].unique().tolist()]\n",
    "\n",
    "print('Number of species in both regions:', len(shared_species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of samples per class and weights for focal loss\n",
    "class_counts = df_audio_meta['species_id'].value_counts().sort_index()\n",
    "class_weights = 1 / class_counts\n",
    "\n",
    "# plot the distribution of the number of samples per class\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "class_counts.sort_values(ascending=False)[0:300].plot(ax=ax, kind='bar', color='blue')\n",
    "ax.set_xticklabels([])\n",
    "ax.tick_params(axis='y', labelcolor='black', labelsize=12, labelfontfamily='serif')\n",
    "ax.set_xlabel('Bird species (sorted by number of samples)', fontsize=14, fontfamily='serif')\n",
    "ax.set_ylabel('Number of Samples', fontsize=14, fontfamily='serif')\n",
    "ax.set_title('Number of Samples per Bird Species [Xeno-Canto]', fontsize=18, fontfamily='serif')\n",
    "ax.set_facecolor('#f3ede2')\n",
    "fig.set_facecolor('#f3ede2')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a sample audio file & spectrogram\n",
    "sample = df_audio_meta.sample(1)\n",
    "\n",
    "audio_file = f\"{config.dataset_dir}/{sample['file_name'].values[0]}\"\n",
    "print(audio_file)\n",
    "\n",
    "y, sr = librosa.load(audio_file, sr=config.audio_sr)\n",
    "\n",
    "meta_str = f\"{sample['common_name'].values[0]} | {sample['scientific_name'].values[0]} | {sample['region'].values[0]}\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9, 4))\n",
    "\n",
    "axes[0].plot(y)\n",
    "axes[0].set_title(meta_str, fontsize=14, fontfamily='serif')\n",
    "axes[0].set_xlabel('Time (samples)', fontsize=12, fontfamily='serif')\n",
    "axes[0].set_ylabel('Amplitude', fontsize=12, fontfamily='serif')\n",
    "\n",
    "mel_spec = ASTFeatureExtractor()(y, sampling_rate=sr, padding=\"max_length\", return_tensors=\"np\")[\"input_values\"]\n",
    "\n",
    "axes[1].imshow(mel_spec[0].T, aspect='auto', origin='lower', cmap='plasma')\n",
    "axes[1].set_xlabel('Time (frames)', fontsize=12, fontfamily='serif')\n",
    "axes[1].set_ylabel('Mel Frequency', fontsize=12, fontfamily='serif')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_facecolor('#f3ede2')\n",
    "    \n",
    "fig.set_facecolor('#f3ede2')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSongDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_audio_meta, config):\n",
    "        self.df_audio_meta = df_audio_meta\n",
    "        self.feature_extractor = ASTFeatureExtractor()\n",
    "        self.config = config\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_audio_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_audio_meta.iloc[idx]\n",
    "        audio_path = f\"{self.config.dataset_dir}/{row['file_name']}\"\n",
    "        audio_arr, sr = self.get_audio(audio_path)\n",
    "        spec = self.feature_extractor(audio_arr, sampling_rate=sr, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return spec['input_values'].squeeze(0), row['species_id']\n",
    "\n",
    "    def get_audio(self, audio_path):\n",
    "        audio, sr = librosa.load(audio_path, sr=self.config.audio_sr)\n",
    "        return audio, sr\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [x[0] for x in batch]\n",
    "    targets = [x[1] for x in batch]\n",
    "    data_dict = {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0),\n",
    "        \"labels\": torch.tensor(targets)\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the dataset and dataloader\n",
    "# bs_dataset = BirdSongDataset(df_audio_meta, config)\n",
    "# bs_dataloader = DataLoader(bs_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# for batch in bs_dataloader:\n",
    "#     print(batch['input_ids'].shape)\n",
    "#     print(batch['labels'])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss_fn and eval_fn\n",
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, gamma=0, alpha=None, reduction='mean', device=DEVICE):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        if isinstance(alpha, (float, int, )): \n",
    "            self.alpha = torch.Tensor([alpha, 1-alpha])\n",
    "        elif isinstance(alpha, (list, np.ndarray)): \n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            raise TypeError('Invalid alpha type')\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=self.alpha, reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = self.ce_loss(logits, targets) # nn.CrossEntropyLoss already applies log_softmax\n",
    "        pt = torch.exp(-ce_loss) # CE = -log(p_t) -> p_t = exp(-CE)\n",
    "        \n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        return_method = {\n",
    "            'mean': torch.mean,\n",
    "            'sum': torch.sum,\n",
    "            'none': lambda x: x\n",
    "        }\n",
    "        \n",
    "        return return_method[self.reduction](focal_loss)\n",
    "        \n",
    "# eval_fn\n",
    "class ROC_AUC_Score(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, average='macro', multi_class='ovo'):\n",
    "        super(ROC_AUC_Score, self).__init__()\n",
    "        self.num_classes = config.n_classes\n",
    "        self.average = average\n",
    "        self.multi_class = multi_class  # 'ovo' (one-vs-one) or 'ovr' (one-vs-rest)\n",
    "        self.label_ids = np.arange(self.num_classes)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (batch_size, n_classes)\n",
    "        # targets: (batch_size,) with integer labels\n",
    "        \n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probas = torch.exp(F.log_softmax(logits, dim=1))\n",
    "\n",
    "        # Detach and move to CPU for sklearn compatibility\n",
    "        probas = probas.detach().cpu().numpy()\n",
    "        targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        df_scores = pd.DataFrame(probas, columns=self.label_ids)\n",
    "        df_scores['target'] = targets\n",
    "        \n",
    "        # remove samples with classes which is predeicted as 0 in all samples\n",
    "        unscored_cols = df_scores.columns[df_scores.sum(axis=0) == 0]\n",
    "        rows_to_remove = df_scores['target'].isin(unscored_cols)\n",
    "        df_scores = df_scores[~rows_to_remove]\n",
    "        \n",
    "        eval_score = roc_auc_score(\n",
    "            y_true=df_scores['target'].values,\n",
    "            y_score=df_scores[self.label_ids].values,\n",
    "            average=self.average, \n",
    "            multi_class=self.multi_class,\n",
    "            labels=self.label_ids\n",
    "        )\n",
    "        \n",
    "        return eval_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test loss_fn and eval_fn\n",
    "# loss_fn = FocalLoss(gamma=2, alpha=class_weights.values, reduction='mean', device='cpu')\n",
    "# eval_fn = ROC_AUC_Score(config, average='macro', multi_class='ovo')\n",
    "\n",
    "# # target_labels = torch.tensor(df_audio_meta['species_id'].sample(10).values)\n",
    "# target_labels = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "# print('Target Labels:', target_labels, \"\\n\")\n",
    "\n",
    "# Test 1: perfect prediction\n",
    "# logits_perfect = F.one_hot(target_labels, num_classes=config.n_classes).float() * 100\n",
    "# loss_perfect = loss_fn(logits_perfect, target_labels)\n",
    "\n",
    "# eval_score_perfect = eval_fn(logits_perfect, target_labels)\n",
    "# print(f\"Perfect Prediction: Loss: {loss_perfect.item()}, Eval Score: {eval_score_perfect}\")\n",
    "# print(\" \")\n",
    "\n",
    "# Test 2: half correct\n",
    "# half_correct = target_labels.clone()\n",
    "# half_correct[:5] = (half_correct[:5] + 1) % config.n_classes  \n",
    "# logits_half = F.one_hot(half_correct, num_classes=config.n_classes).float() * 100\n",
    "# logits_half[:5] *= -1  \n",
    "# loss_half = loss_fn(logits_half, target_labels)\n",
    "\n",
    "# eval_score_half = eval_fn(logits_half, target_labels)\n",
    "# print(f\"Half Correct Prediction: Loss: {loss_half.item()}, Eval Score: {eval_score_half}\")\n",
    "# print(\" \")\n",
    "\n",
    "# Test 3: random prediction\n",
    "# logits_random = torch.rand((10, config.n_classes))\n",
    "# loss_random = loss_fn(logits_random, target_labels)\n",
    "\n",
    "# eval_score_random = eval_fn(logits_random, target_labels)\n",
    "# print(f\"Random Prediction: Loss: {loss_random.item()}, Eval Score: {eval_score_random}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdAST(nn.Module):\n",
    "    \n",
    "    def __init__(self, backbone_name, n_classes, n_mlp_layers=1, activation='silu'):\n",
    "        super(BirdAST, self).__init__()\n",
    "        \n",
    "        # pre-trained backbone\n",
    "        backbone_config = ASTConfig.from_pretrained(backbone_name)\n",
    "        self.ast = ASTModel.from_pretrained(backbone_name, config=backbone_config)\n",
    "        self.hidden_size = backbone_config.hidden_size\n",
    "        \n",
    "        # set activation functions\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'silu':\n",
    "            self.activation = nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function. Choose 'relu' or 'silu'.\")\n",
    "        \n",
    "        # define MLP layers with activation\n",
    "        layers = []\n",
    "        for _ in range(n_mlp_layers):\n",
    "            layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            layers.append(self.activation)\n",
    "        layers.append(nn.Linear(self.hidden_size, n_classes))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, spectrogram):\n",
    "        # spectrogram: (batch_size, n_mels, n_frames)\n",
    "        # output: (batch_size, n_classes)\n",
    "        \n",
    "        ast_output = self.ast(spectrogram, output_hidden_states=False)\n",
    "        logits = self.mlp(ast_output.last_hidden_state[:, 0, :]) # Use the CLS token \n",
    "        \n",
    "        return {'logits': logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the model / loss_fn / eval_fn using dataloader\n",
    "\n",
    "# bs_dataset = BirdSongDataset(df_audio_meta, config)\n",
    "# bs_dataloader = DataLoader(bs_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# loss_fn = FocalLoss(gamma=2, alpha=class_weights.values, reduction='mean', device='cpu')\n",
    "# eval_fn = ROC_AUC_Score(label_ids_list, average='macro')\n",
    "\n",
    "# bird_ast_model = BirdAST(config.backbone_name, config.n_classes, n_mlp_layers=1, activation='silu')\n",
    "\n",
    "# for batch in bs_dataloader:\n",
    "    \n",
    "#     input_ids = batch['input_ids']\n",
    "#     labels = batch['labels']\n",
    "    \n",
    "#     output = bird_ast_model(input_ids)\n",
    "#     logits = output['logits']\n",
    "    \n",
    "#     loss = loss_fn(logits, labels)\n",
    "#     eval_score = eval_fn(logits, labels)\n",
    "    \n",
    "#     print(f\"Shape of input_ids: {input_ids.shape}\")\n",
    "#     print(f\"Shape of labels: {labels.shape}\")\n",
    "#     print(f\"Shape of logits: {logits.shape}\")\n",
    "#     print(f\"Loss: {loss.item()}, Eval Score: {eval_score}\")\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.value = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.value = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, loss_fn, evel_fn, logger, config, is_higher_better=True):\n",
    "        '''\n",
    "        model: nn.Module\n",
    "        loss_fn: loss function\n",
    "        evel_fn: evaluation function\n",
    "        logger: logger\n",
    "        config: Config\n",
    "        is_higher_better: bool (default: True) whether higher evaluation score is better\n",
    "        '''\n",
    "\n",
    "        self.model = model\n",
    "        self.logger = logger\n",
    "        self.config = config\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_fn.to(self.device)\n",
    "        \n",
    "        self.eval_fn = evel_fn\n",
    "        self.is_higher_better = is_higher_better\n",
    "        \n",
    "    def train(self, train_loader, valid_loader, print_epoch_freq=50, from_checkpoint=None, use_tqdm=True):\n",
    "\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=1e-3, weight_decay=self.config.weight_decay)\n",
    "\n",
    "        self.scheduler = OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.config.max_lr,\n",
    "            epochs=self.config.epochs,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy=\"cos\",\n",
    "            final_div_factor=self.config.lr_final_div,\n",
    "        )\n",
    "\n",
    "        if from_checkpoint is not None:\n",
    "            self.model.load_state_dict(torch.load(from_checkpoint, map_location=self.device))\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        best_weights, best_preds, best_loss = None, None, float(\"-inf\") if self.is_higher_better else float(\"inf\")\n",
    "        loss_records = {\"train\": [], \"valid\": []}\n",
    "        \n",
    "        if use_tqdm:\n",
    "            pbar = tqdm(range(self.config.epochs), total=self.config.epochs, unit=\"epoch\", leave=True, desc=\"Training Progress\")\n",
    "        else:\n",
    "            pbar = range(self.config.epochs)\n",
    "            \n",
    "        for epoch in pbar:\n",
    "            start_epoch = time()\n",
    "\n",
    "            train_loss, _ = self._train_or_valid_epoch(epoch, train_loader, is_train=True, use_tqdm=use_tqdm)\n",
    "            valid_loss, valid_preds = self._train_or_valid_epoch(epoch, valid_loader, is_train=False, use_tqdm=use_tqdm)\n",
    "            \n",
    "            loss_records[\"train\"].append(train_loss)\n",
    "            loss_records[\"valid\"].append(valid_loss)\n",
    "\n",
    "            elapsed = time() - start_epoch\n",
    "            \n",
    "            if (epoch % print_epoch_freq == 0) or (epoch == (self.config.epochs - 1)):\n",
    "                self.logger.info(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f} - Valid Loss: {valid_loss:.4f} - Elapsed Time: {elapsed:.2f}s\")\n",
    "            \n",
    "            if self.is_higher_better:\n",
    "                if valid_loss > best_loss:\n",
    "                    best_loss = valid_loss\n",
    "                    best_weights = self.model.state_dict()\n",
    "                    best_preds = valid_preds\n",
    "                    self.logger.info(f\"- Epoch {epoch + 1}: Best model found with loss = {best_loss:.4f}.\")\n",
    "            else:\n",
    "                if valid_loss < best_loss:\n",
    "                    best_loss = valid_loss\n",
    "                    best_weights = self.model.state_dict()\n",
    "                    best_preds = valid_preds\n",
    "                    self.logger.info(f\"- Epoch {epoch + 1}: Best model found with loss = {best_loss:.4f}.\")\n",
    "\n",
    "        return best_weights, best_preds, loss_records\n",
    "\n",
    "    def _train_or_valid_epoch(self, epoch_id, dataloader, is_train=True, use_tqdm=True):\n",
    "\n",
    "        self.model.train() if is_train else self.model.eval()\n",
    "        mode = \"Train\" if is_train else \"Valid\"\n",
    "\n",
    "        len_loader = len(dataloader)\n",
    "        scaler = GradScaler(enabled=self.config.amp)\n",
    "        loss_meter = AverageMeter()\n",
    "        labels_record, predicts_record = [], []\n",
    "\n",
    "        start = time()\n",
    "        \n",
    "        if use_tqdm:\n",
    "            pbar = tqdm(enumerate(dataloader), total=len_loader, desc=mode, unit=\"batch\")\n",
    "        else:\n",
    "            pbar = enumerate(dataloader)\n",
    "            \n",
    "        for step, data_dict in pbar:\n",
    "            \n",
    "            input_ids = data_dict['input_ids'].to(self.device)\n",
    "            labels = data_dict['labels'].to(self.device)\n",
    "            \n",
    "            if is_train:\n",
    "                with autocast(enabled=self.config.amp):\n",
    "                    model_output = self.model(input_ids)\n",
    "                    logits = model_output['logits']\n",
    "                    train_loss = self.loss_fn(logits, labels)\n",
    "                    \n",
    "                if self.config.grad_accum_steps > 1:\n",
    "                    train_loss = train_loss / self.config.grad_accum_steps\n",
    "                    \n",
    "                scaler.scale(train_loss).backward()\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                if (step + 1) % self.config.grad_accum_steps == 0:\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                loss_meter.update(train_loss.item())\n",
    "                \n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(input_ids)\n",
    "                    logits = model_output['logits']\n",
    "                    valid_loss = self.loss_fn(logits, labels)\n",
    "\n",
    "                if self.config.grad_accum_steps > 1:\n",
    "                    valid_loss = valid_loss / self.config.grad_accum_steps\n",
    "                    \n",
    "                loss_meter.update(valid_loss.item())\n",
    "                \n",
    "                predicts_record.append(logits)\n",
    "                labels_record.append(labels)\n",
    "            \n",
    "            end = time()\n",
    "            \n",
    "            if self.config.print_freq:\n",
    "                if (step % self.config.print_freq == 0) or (step == (len_loader - 1)):\n",
    "                    lr = self.scheduler.get_last_lr()[0]\n",
    "                    info = f\"Epoch {epoch_id + 1} [{step}/{len_loader}] | {mode} \"\n",
    "                    \n",
    "                    if is_train:\n",
    "                        info += f\"Loss: {loss_meter.avg:.4f} Grad: {grad_norm:.4f} LR: {lr:.4e}\"\n",
    "                    else:\n",
    "                        info += f\"Loss: {loss_meter.avg:.4f}\"\n",
    "\n",
    "                    info += f\" | Elapse: {end - start:.2f}s\"\n",
    "                    self.logger.info(info)\n",
    "\n",
    "        if is_train:\n",
    "            wandb.log({\n",
    "                \"Train Loss\": loss_meter.avg, \n",
    "                \"Learning Rate\": self.scheduler.get_last_lr()[0],\n",
    "                \"Gradient Norm\": grad_norm,\n",
    "                \"Epoch\": epoch_id + 1\n",
    "            })\n",
    "            return loss_meter.avg, None\n",
    "        else:\n",
    "            eval_loss = self.eval_fn(\n",
    "                torch.cat(predicts_record).cpu(),\n",
    "                torch.cat(labels_record).cpu()\n",
    "                )\n",
    "            wandb.log({\n",
    "                \"Valid Loss\": loss_meter.avg,\n",
    "                \"Eval Score\": eval_loss,\n",
    "                \"Epoch\": epoch_id + 1\n",
    "            })\n",
    "            predicts_record = np.concatenate([p.cpu().detach().numpy() for p in predicts_record], axis=0)\n",
    "            return eval_loss, predicts_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_record(loss_history, final_loss, start_at=0, save_to=None):\n",
    "    \n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    for i, loss in enumerate(loss_history):\n",
    "        ax1.plot(np.arange(start_at, len(loss['train'])), loss['train'][start_at:], \"-\",   label=f'Train_{i}')\n",
    "        ax2.plot(np.arange(start_at, len(loss['valid'])), loss['valid'][start_at:], \".--\", label=f'Valid_{i}')\n",
    "        \n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Train Loss\")\n",
    "    ax2.set_ylabel(\"Valid Loss (Dashed)\")\n",
    "    ax1.set_title(f\"Final Eval-Loss: {final_loss:.4f}\")\n",
    "    ax1.legend(loc='lower left', bbox_to_anchor=[1.15, 0])\n",
    "    ax2.legend(loc='upper left', bbox_to_anchor=[1.15, 1])\n",
    "    \n",
    "    if save_to:\n",
    "        plt.savefig(save_to)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=config.n_splits)\n",
    "df_audio_meta['fold'] = 0\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(gkf.split(df_audio_meta, groups=df_audio_meta['group_id'])):\n",
    "    df_audio_meta.loc[test_index, 'fold'] = fold\n",
    "    \n",
    "display(df_audio_meta['fold'].value_counts())\n",
    "\n",
    "# check if any sample in valid set has the same mp3_link as the train set\n",
    "for fold in range(config.n_splits):\n",
    "    train_group_ids = df_audio_meta[df_audio_meta['fold'] != fold]['mp3_link'].unique()\n",
    "    valid_group_ids = df_audio_meta[df_audio_meta['fold'] == fold]['mp3_link'].unique()\n",
    "    common_groups = set(train_group_ids).intersection(set(valid_group_ids))\n",
    "    print(f\"Fold {fold}: Number of common groups between train and valid: {len(common_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = FocalLoss(gamma=2, alpha=class_weights.values, reduction='mean', device='cpu')\n",
    "eval_fn = ROC_AUC_Score(config, average='macro', multi_class='ovo')\n",
    "\n",
    "oof, loss_history = pd.DataFrame(), []\n",
    "\n",
    "logger = get_logger(log_file=f\"{config.log_dir}/{config.model_name}_training.log\")\n",
    "\n",
    "# log TRAIN_CONFIG\n",
    "logger.info(f\"{'#'*35} TRAIN_CONFIG {'#'*35}\")\n",
    "for k, v in config.__dict__.items():\n",
    "    if '__' not in k:\n",
    "        logger.info(f\"{k}: {v}\")\n",
    "\n",
    "logger.info(f\"{'#'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Training Loop Starts Here\n",
    "# --------------------------------------------------------------\n",
    "loader_kwargs = {\n",
    "    \"batch_size\": config.batch_size,\n",
    "    \"num_workers\": config.num_workers,\n",
    "    \"pin_memory\": True,\n",
    "    \"shuffle\": False,\n",
    "    'collate_fn': collate_fn\n",
    "}\n",
    "\n",
    "prediction_columns = [f\"pred_{i}\" for i in range(config.n_classes)]\n",
    "    \n",
    "for fold_id in range(0, config.n_splits):\n",
    "    tik = time()\n",
    "    \n",
    "    wandb_init(project_name=config.model_name, run_name=f\"fold_{fold_id}\", config=config)\n",
    "    \n",
    "    train_df = df_audio_meta[df_audio_meta['fold'] != fold_id].copy().reset_index(drop=True)\n",
    "    valid_df = df_audio_meta[df_audio_meta['fold'] == fold_id].copy().reset_index(drop=True)\n",
    "    \n",
    "    train_folds = BirdSongDataset(train_df, config)\n",
    "    valid_folds = BirdSongDataset(valid_df, config)\n",
    "    \n",
    "    train_loader = DataLoader(train_folds, **loader_kwargs)\n",
    "    valid_loader = DataLoader(valid_folds, **loader_kwargs)\n",
    "        \n",
    "    model = BirdAST(config.backbone_name, config.n_classes, n_mlp_layers=1, activation='silu')\n",
    "    \n",
    "    trainer = Trainer(model, loss_fn, eval_fn, logger, config, is_higher_better=True)\n",
    "    \n",
    "    best_weights, best_preds, loss_records = trainer.train( \n",
    "        train_loader, \n",
    "        valid_loader, \n",
    "        print_epoch_freq=config.print_epoch_freq,\n",
    "        from_checkpoint=None,\n",
    "        use_tqdm=True\n",
    "        )\n",
    "    \n",
    "    loss_history.append(loss_records)\n",
    "    \n",
    "    df_valid = pd.DataFrame({'species_id': valid_df['species_id'], 'fold': fold})\n",
    "    df_valid[prediction_columns] = best_preds\n",
    "    \n",
    "    oof = pd.concat([oof, df_valid], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    save_model_name = f\"{config.model_name}_fold_{fold_id}\"\n",
    "    torch.save(best_weights, f\"{config.log_dir}/{save_model_name}.pth\")\n",
    "\n",
    "    del train_folds, valid_folds, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    valid_loss_fold = eval_fn(\n",
    "        torch.tensor(oof[prediction_columns].values),\n",
    "        torch.tensor(oof['species_id'].values)\n",
    "    )\n",
    "    logger.info(f\"Fold {fold_id} | Time: {(time() - tik)/60:.2f}min | Overall Evaluation Loss: {valid_loss_fold:.4f}\")\n",
    "    \n",
    "    # Save the oof predictions\n",
    "    oof.to_csv(f\"{config.log_dir}/{config.model_name}_oof.csv\", index=False)\n",
    "    \n",
    "    wandb.finish()\n",
    "        \n",
    "# Summarize the final results\n",
    "valid_loss = eval_fn(\n",
    "    torch.tensor(oof[prediction_columns].values),\n",
    "    torch.tensor(oof['species_id'].values)\n",
    ")\n",
    "\n",
    "plot_loss_record(loss_history, valid_loss, start_at=0, save_to=os.path.join(config.log_dir, f\"{config.model_name}_loss.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a hidden layer of the model\n",
    "\n",
    "model = BirdAST(config.backbone_name, config.n_classes, n_mlp_layers=1, activation='silu')\n",
    "model.load_state_dict(torch.load(f\"{config.log_dir}/{config.model_name}_fold_0.pth\"))\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "sample_dataset = BirdSongDataset(df_audio_meta, config)\n",
    "sample_dataloader = DataLoader(sample_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for batch in sample_dataloader:\n",
    "    \n",
    "    input_ids = batch['input_ids'].to(DEVICE)\n",
    "    labels = batch['labels'].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(input_ids)\n",
    "        logits = model_output['logits']\n",
    "        hidden_states = model.ast(input_ids, output_hidden_states=True) #['hidden_states']\n",
    "        \n",
    "    break\n",
    "\n",
    "last_hidden_state = hidden_states.last_hidden_state.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(last_hidden_state[0].T, aspect='auto', origin='lower', cmap='plasma') #, vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "\n",
    "ax = axes[1]\n",
    "arr = last_hidden_state[0].T\n",
    "ax.plot(arr.max(axis=0), label='Mean')\n",
    "# ax.plot(arr[3], label='First')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse training log\n",
    "import re \n",
    "\n",
    "log_file = f\"{config.log_dir}/{config.model_name}_training.log\"\n",
    "\n",
    "with open(log_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "epoch_records = []\n",
    "epoch_pattern = re.compile(r\"Epoch (\\d+) - Train Loss: (\\d+\\.\\d+) - Valid Loss: (\\d+\\.\\d+) - Elapsed Time: (\\d+\\.\\d+)s\")\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    try:\n",
    "        epoch, train_loss, valid_loss, elapsed = epoch_pattern.findall(line)[0]\n",
    "        epoch_records.append({\n",
    "            \"epoch\": int(epoch),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"valid_loss\": float(valid_loss),\n",
    "            \"elapsed\": float(elapsed)\n",
    "        })\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "df_epoch_records = pd.DataFrame(epoch_records)\n",
    "\n",
    "df_epoch_records['fold_id'] = df_epoch_records.index // 5\n",
    "\n",
    "df_epoch_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze oof predictions\n",
    "df_oof = pd.read_csv(f\"{config.log_dir}/{config.model_name}_oof.csv\")\n",
    "df_oof['species_id'] = df_oof['species_id'].astype(int)\n",
    "\n",
    "species_category = pd.cut(\n",
    "    df_oof['species_id'].value_counts(), \n",
    "    bins=[0, 50, 120], \n",
    "    labels=['0-50', '50-120']\n",
    "    ).to_frame()\n",
    "\n",
    "species_category['count']\n",
    "\n",
    "df_oof['species_category'] = df_oof['species_id'].map({i: c for i, c in enumerate(species_category['count'])})\n",
    "\n",
    "prediction_columns = [f\"pred_{i}\" for i in range(config.n_classes)]\n",
    "\n",
    "display(df_oof.head())\n",
    "\n",
    "# eval_fn = ROC_AUC_Score(config, average='macro', multi_class='ovo')\n",
    "\n",
    "# # caculate the overall evaluation loss\n",
    "# eval_loss = eval_fn(\n",
    "#     torch.tensor(df_oof[prediction_columns].values),\n",
    "#     torch.tensor(df_oof['species_id'].values)\n",
    "# )\n",
    "\n",
    "# print(f\"Overall Evaluation Loss: {eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_oof['species_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cm plots\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, save_to=None):\n",
    "    \n",
    "    sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'}) #f3ede2\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=False, cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    # plt.xticks([])\n",
    "    # plt.yticks([])\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save_to:\n",
    "        plt.savefig(save_to)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "y_true = df_oof['species_id'].values\n",
    "y_pred = df_oof[prediction_columns].values.argmax(axis=1)\n",
    "\n",
    "labels = df_oof['species_id'].unique()\n",
    "labels.sort()\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    y_true, y_pred, \n",
    "    labels=labels,\n",
    "    save_to=os.path.join(config.log_dir, f\"{config.model_name}_cm.jpg\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.random.choice(len(df_oof['species_id'].unique()), 50)\n",
    "labels.sort()\n",
    "\n",
    "tmp = df_oof[df_oof['species_id'].isin(labels)].copy()\n",
    "# plot confusion matrix\n",
    "y_true = tmp['species_id'].values\n",
    "y_pred = tmp[prediction_columns].values.argmax(axis=1)\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, labels=labels, save_to=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc F1, Precision, Recall, mAP \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(species_category['count'].unique())\n",
    "\n",
    "for category_id in ['0-50', '50-120']:\n",
    "    tmp = df_oof[df_oof['species_category'] == category_id].copy().reset_index(drop=True)\n",
    "\n",
    "    y_true = tmp['species_id'].values\n",
    "    y_pred = tmp[prediction_columns].values.argmax(axis=1)\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Category: {category_id} | N_samples: {len(tmp)} | N_classes: {tmp['species_id'].nunique()} | F1: {f1} | Precision: {precision} | Recall: {recall:.4f}\")\n",
    "\n",
    "    # labels = tmp['species_id'].unique()\n",
    "    # cm = confusion_matrix(y_true, y_pred, labels=labels, normalize='pred')\n",
    "    \n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # sns.heatmap(cm, annot=False, cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    # plt.xlabel(\"Predicted Label\")\n",
    "    # plt.ylabel(\"True Label\")\n",
    "    # plt.title(\"Normalized Confusion Matrix | Category: \" + category_id)\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot training progress\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(9, 6))\n",
    "\n",
    "# ax_right = ax.twinx()\n",
    "\n",
    "# for fold_id in range(config.n_splits):\n",
    "#     df_fold = df_epoch_records[df_epoch_records['fold_id'] == fold_id]\n",
    "#     ax.plot(df_fold['epoch'], df_fold['train_loss'], marker='o', label=f\"Train_Fold_{fold_id}\")\n",
    "#     ax_right.plot(df_fold['epoch'], df_fold['valid_loss'], linestyle=\"--\", marker='*', label=f\"Valid_Fold_{fold_id}\")\n",
    "\n",
    "# ax.set_xticks(np.arange(0, config.epochs, 1)+1)\n",
    "# ax.set_xlabel(\"Epoch\", fontsize=14, fontfamily='serif')\n",
    "# ax.set_ylabel(\"Training Loss (Focal Loss)\", fontsize=14, fontfamily='serif')\n",
    "# ax.set_title(f\"Training Progress | Overall ROCAUC: {eval_loss:.3f}\", fontsize=18, fontfamily='serif')\n",
    "# ax.legend(loc='upper left')\n",
    "\n",
    "# ax_right.set_ylabel(\"Validation Loss (ROCAUC)\", fontsize=14, fontfamily='serif')\n",
    "# ax_right.legend(loc='lower left')\n",
    "\n",
    "# ax.set_facecolor('#f3ede2')\n",
    "# fig.set_facecolor('#f3ede2')\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
